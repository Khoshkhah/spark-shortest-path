{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "bd1fce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-21-openjdk-amd64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "fead58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "import h3\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab1e65",
   "metadata": {},
   "source": [
    "## 1- Initialize spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "7af38781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_spark(app_name: str = \"AllPairsShortestPath\") -> SparkSession:\n",
    "    \"\"\"\n",
    "    Initializes and returns a SparkSession.\n",
    "    \"\"\"\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(app_name)\n",
    "        .config(\"spark.driver.memory\", \"8g\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    spark.sparkContext.setCheckpointDir(\"checkpoints\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49abb778",
   "metadata": {},
   "source": [
    "## 2- Read edges data and initialize shortcuts table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "cf6bbba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_edges(spark: SparkSession, file_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Reads edges from a CSV file into a DataFrame.\n",
    "    \"\"\"\n",
    "    edges_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    edges_df = edges_df.select(\"id\", \"incoming_cell\", \"outgoing_cell\", \"lca_res\")\n",
    "    return edges_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "33de3f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_shortcuts_table(spark: SparkSession, file_path: str, edges_cost_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Creates the initial shortcuts table from the edges DataFrame.\n",
    "    \"\"\"\n",
    "    shortcuts_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    shortcuts_df = shortcuts_df.select(\"incoming_edge\", \"outgoing_edge\")\n",
    "    # Add new via_edge column\n",
    "    shortcuts_df = shortcuts_df.withColumn(\"via_edge\", F.col(\"outgoing_edge\"))\n",
    "    \n",
    "    # shortcuts_df.show(5)\n",
    "    shortcuts_df = shortcuts_df.join(\n",
    "        edges_cost_df.select(\"id\", \"cost\"), \n",
    "        shortcuts_df.incoming_edge == edges_cost_df.id, \n",
    "        \"left\"\n",
    "    ).drop(edges_cost_df.id)\n",
    "   \n",
    "    return shortcuts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c691c3",
   "metadata": {},
   "source": [
    "## 3- Update edges cost by dummy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "af10a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=DoubleType())\n",
    "def dummy_cost(length, maxspeed):\n",
    "    \"\"\"Calculates cost based on length and maxspeed.\"\"\"\n",
    "    if maxspeed <= 0:\n",
    "        return float('inf')  # Return infinity for invalid speeds\n",
    "    # Example calculation: Cost is proportional to length and inversely proportional to speed\n",
    "    return float(length) / float(maxspeed)\n",
    "\n",
    "def update_dummy_costs_for_edges(spark: SparkSession, file_path: str, edges_df: DataFrame):\n",
    "    \"\"\"\n",
    "    Adds a dummy cost column to the edges DataFrame.\n",
    "    \"\"\"\n",
    "    edges_df_cost = spark.read.csv(file_path, header=True, inferSchema=True).select(\"id\", \"length\", \"maxspeed\")\n",
    "    edges_df_cost = edges_df_cost.withColumn(\n",
    "        \"cost\", \n",
    "        dummy_cost(F.col(\"length\"), F.col(\"maxspeed\"))\n",
    "    )\n",
    "    edges_df = edges_df.drop(\"cost\")  # Remove existing cost column if present\n",
    "    edges_df = edges_df.join(edges_df_cost.select(\"id\", \"cost\"), on=\"id\", how=\"left\")\n",
    "    return edges_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1700d1",
   "metadata": {},
   "source": [
    "## 4- Add info \"via_cell\", \"via_res\" and \"lca_res\" to shortcuts table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "419b68d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCA - Lowest Common Ancestor resolution\n",
    "@F.udf(StringType())\n",
    "def find_LCA(cell1, cell2):\n",
    "    cell1_res = h3.get_resolution(cell1)\n",
    "    cell2_res = h3.get_resolution(cell2)\n",
    "    lca_res = min(cell1_res, cell2_res)\n",
    "    while h3.cell_to_parent(cell1, lca_res) != h3.cell_to_parent(cell2, lca_res) and lca_res > 0:\n",
    "        lca_res -= 1\n",
    "    mycell = h3.cell_to_parent(cell1, lca_res)\n",
    "    if mycell == h3.cell_to_parent(cell2, lca_res):\n",
    "        return mycell\n",
    "    else:\n",
    "        return None  # maybe return 0 is better option\n",
    "\n",
    "# Resolution of a cell\n",
    "@F.udf(IntegerType())  \n",
    "def find_resolution(cell):\n",
    "    if cell is None:\n",
    "        return -1\n",
    "    return h3.get_resolution(cell)\n",
    "\n",
    "def add_info_for_shortcuts(spark: SparkSession, shortcuts_df: DataFrame, edges_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adds additional information to the shortcuts DataFrame.\n",
    "    Add incoming_cell and lca_res from incoming_edge to shortcuts_df\n",
    "    Add outgoing_cell and lca_res from outgoing_edge to shortcuts_df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop existing columns if present: lca_res, via_cell, via_res\n",
    "    for col in [\"lca_res\", \"via_cell\", \"via_res\"]:\n",
    "        if col in shortcuts_df.columns:\n",
    "            shortcuts_df = shortcuts_df.drop(col)\n",
    "            \n",
    "    # Join to get incoming_cell and lca for incoming_edge\n",
    "    shortcuts_df = shortcuts_df.join(\n",
    "        edges_df.select(\n",
    "            F.col(\"id\").alias(\"incoming_edge_id\"),\n",
    "            F.col(\"incoming_cell\").alias(\"incoming_cell_in\"),\n",
    "            F.col(\"lca_res\").alias(\"lca_res_in\")\n",
    "        ),\n",
    "        shortcuts_df.incoming_edge == F.col(\"incoming_edge_id\"),\n",
    "        \"left\"\n",
    "    ).drop(\"incoming_edge_id\")\n",
    "    \n",
    "    # Join to get outgoing_cell and lca for outgoing_edge\n",
    "    shortcuts_df = shortcuts_df.join(\n",
    "        edges_df.select(\n",
    "            F.col(\"id\").alias(\"outgoing_edge_id\"),\n",
    "            F.col(\"outgoing_cell\").alias(\"outgoing_cell_out\"),\n",
    "            F.col(\"lca_res\").alias(\"lca_res_out\")\n",
    "        ),\n",
    "        shortcuts_df.outgoing_edge == F.col(\"outgoing_edge_id\"),\n",
    "        \"left\"\n",
    "    ).drop(\"outgoing_edge_id\")\n",
    "    \n",
    "    # Add lca_res column as the maximum of lca_res_in and lca_res_out\n",
    "    shortcuts_df = shortcuts_df.withColumn(\n",
    "        \"lca_res\",\n",
    "        F.greatest(F.col(\"lca_res_in\"), F.col(\"lca_res_out\"))\n",
    "    )\n",
    "    # Drop intermediate lca_res_in and lca_res_out columns\n",
    "    shortcuts_df = shortcuts_df.drop(\"lca_res_in\", \"lca_res_out\")\n",
    "    \n",
    "    # Add via_cell column as LCA of incoming_cell_in and outgoing_cell_out\n",
    "    shortcuts_df = shortcuts_df.withColumn(\n",
    "        \"via_cell\", find_LCA(F.col(\"incoming_cell_in\"), F.col(\"outgoing_cell_out\"))\n",
    "    )   \n",
    "    shortcuts_df = shortcuts_df.withColumn(\n",
    "        \"via_res\", find_resolution(F.col(\"via_cell\"))\n",
    "    )\n",
    "    # Drop intermediate incoming_cell_in and outgoing_cell_out columns\n",
    "    shortcuts_df = shortcuts_df.drop(\"incoming_cell_in\", \"outgoing_cell_out\")   \n",
    "    \n",
    "    return shortcuts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7abc15f",
   "metadata": {},
   "source": [
    "## 5- Filter shortcuts table based on current resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "d02c11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_shortcuts_by_resolution(shortcuts_df: DataFrame, current_res: int) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the shortcuts DataFrame based on the current resolution.\n",
    "    Keeps only the shortcuts where lca_res is greater than or equal to current_res.\n",
    "    \"\"\"\n",
    "    filtered_shortcuts_df = shortcuts_df.filter(\n",
    "        (F.col(\"lca_res\") <= current_res) & (F.col(\"via_res\") >= current_res)\n",
    "    )\n",
    "    return filtered_shortcuts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f7da0",
   "metadata": {},
   "source": [
    "## 6- add cell container for each shortcut based on current resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "d8a6c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_parent_cell_at_resolution(shortcuts_df: DataFrame, current_resolution: int) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a cell container column to shortcuts based on the current resolution.\n",
    "    \n",
    "    The cell container is computed as the parent cell of the via_cell at the \n",
    "    current resolution level. This allows grouping shortcuts by their spatial\n",
    "    containment at different H3 resolution levels.\n",
    "    \n",
    "    Args:\n",
    "        shortcuts_df: DataFrame with shortcuts containing via_cell column\n",
    "        current_resolution: The H3 resolution level to use for cell containers\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added 'current_cell' column containing the parent cell\n",
    "    \"\"\"\n",
    "    \n",
    "    @F.udf(StringType())\n",
    "    def get_parent_cell(cell, target_resolution):\n",
    "        \"\"\"\n",
    "        Get the parent cell of a given cell at the target resolution.\n",
    "        Returns None if the cell is None or if the resolution is invalid.\n",
    "        \"\"\"\n",
    "        if cell is None:\n",
    "            return None\n",
    "        try:\n",
    "            cell_res = h3.get_resolution(cell)\n",
    "            # If current resolution is higher than cell resolution, return the cell itself\n",
    "            if target_resolution >= cell_res:\n",
    "                return cell\n",
    "            # Otherwise, get the parent at the target resolution\n",
    "            return h3.cell_to_parent(cell, target_resolution)\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    # Add the current_cell column based on via_cell\n",
    "    shortcuts_df = shortcuts_df.withColumn(\n",
    "        \"current_cell\",\n",
    "        get_parent_cell(F.col(\"via_cell\"), F.lit(current_resolution))\n",
    "    )\n",
    "    shortcuts_df = shortcuts_df.drop(\"lca_res\", \"via_cell\", \"via_res\")\n",
    "    # Filter out rows where current_cell is None (invalid cells)\n",
    "    # shortcuts_df = shortcuts_df.filter(F.col(\"current_cell\").isNotNull())\n",
    "    \n",
    "    return shortcuts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa65f7c",
   "metadata": {},
   "source": [
    "# 7- shortest path in pure spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "bed6f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def update_convergence_status(\n",
    "    shortcuts_df_last: DataFrame,\n",
    "    shortcuts_df_new: DataFrame\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Overwrites the 'is_converged' column in shortcuts_df_new.\n",
    "    \n",
    "    A 'current_cell' group is marked True if none of its associated rows \n",
    "    (based on the three join keys) have changed between 'last' and 'new'.\n",
    "    \n",
    "    The resulting DataFrame retains all original columns, including the updated 'is_converged'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The keys that define a shortcut's identity for change detection\n",
    "    join_keys = [\"incoming_edge\", \"outgoing_edge\", \"cost\"]\n",
    "\n",
    "    # --- 1. Identify all ACTIVE (Non-Converged) current_cell IDs ---\n",
    "    \n",
    "    # Use LEFT ANTI-JOIN to find rows in 'new' that are new or changed\n",
    "    non_converged_rows = shortcuts_df_new.join(\n",
    "        shortcuts_df_last,\n",
    "        on=join_keys,\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "\n",
    "    # Extract the unique 'current_cell' IDs that are involved in a change.\n",
    "    active_cell_ids = non_converged_rows.select(\"current_cell\").distinct().alias(\"active\")\n",
    "    \n",
    "    # --- 2. Flag the Final DataFrame (Overwrite existing 'is_converged') ---\n",
    "\n",
    "    # Start with the original new DataFrame\n",
    "    result_df = shortcuts_df_new.alias(\"new\")\n",
    "\n",
    "    # Perform a LEFT OUTER join with the list of ACTIVE cell IDs.\n",
    "    # We join on \"current_cell\" because convergence is defined at the cell group level.\n",
    "    result_df = result_df.join(\n",
    "        active_cell_ids.alias(\"active\"),\n",
    "        on=\"current_cell\",\n",
    "        how=\"left_outer\"\n",
    "    )\n",
    "    \n",
    "    # Update the 'is_converged' column:\n",
    "    # If the current_cell ID matched an 'active' ID (col is not null), it's NOT converged (False).\n",
    "    # If the current_cell ID did not match (col is null), it IS converged (True).\n",
    "    final_df = result_df.withColumn(\n",
    "        \"is_converged\",\n",
    "        F.when(\n",
    "            F.col(\"active.current_cell\").isNull(),\n",
    "            F.lit(True)\n",
    "        ).otherwise(\n",
    "            F.lit(False)\n",
    "        )\n",
    "    ).drop(\"active.current_cell\") # Clean up the temporary join column used for the flag\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "39d5f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# NOTE: The necessary 'update_convergence_status' function definition is assumed to exist.\n",
    "\n",
    "def run_grouped_shortest_path_with_convergence(shortcuts_df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    \n",
    "    # 1. INITIALIZE: Start with the original edges (shortcuts_df)\n",
    "    current_paths = shortcuts_df.select(\n",
    "        \"incoming_edge\", \"outgoing_edge\", \"cost\", \"via_edge\", \"current_cell\",\n",
    "    )\n",
    "    current_paths = current_paths.withColumn(\n",
    "        \"is_converged\",\n",
    "        F.lit(False)\n",
    "    ).cache()\n",
    "\n",
    "    #while True:\n",
    "    for i in range(4):\n",
    "        # --- 2. PATH EXTENSION (Find new, two-hop paths) ---\n",
    "        new_paths = current_paths.alias(\"L\").join(\n",
    "            current_paths.alias(\"R\"),\n",
    "            # CRITICAL CORRECTION: Join on connection AND same cell group\n",
    "            [\n",
    "                F.col(\"L.outgoing_edge\") == F.col(\"R.incoming_edge\"),\n",
    "                F.col(\"L.current_cell\") == F.col(\"R.current_cell\")\n",
    "            ],\n",
    "            \"inner\"\n",
    "        ).filter(\n",
    "            # 1. No self-loops for the entire path\n",
    "            (F.col(\"L.incoming_edge\") != F.col(\"R.outgoing_edge\")) &\n",
    "            # 2. Only extend paths from non-converged edges\n",
    "            (F.col(\"L.is_converged\") == F.lit(False)) &\n",
    "            (F.col(\"R.is_converged\") == F.lit(False))\n",
    "        ).select(\n",
    "            F.col(\"L.incoming_edge\").alias(\"incoming_edge\"),\n",
    "            F.col(\"R.outgoing_edge\").alias(\"outgoing_edge\"),\n",
    "            (F.col(\"L.cost\") + F.col(\"R.cost\")).alias(\"cost\"),\n",
    "            F.col(\"L.outgoing_edge\").alias(\"via_edge\"),\n",
    "            F.col(\"L.current_cell\").alias(\"current_cell\"),\n",
    "            F.lit(False).alias(\"is_converged\"), # New path is NOT converged\n",
    "        ).cache()\n",
    "\n",
    "        if new_paths.limit(1).count()==0:\n",
    "            print(\"======= len new_path is zero!! ==========\")\n",
    "            break\n",
    "        # --- 3. COST MINIMIZATION (Union and GroupBy) ---\n",
    "        all_paths = current_paths.unionByName(new_paths)\n",
    "        ########\n",
    "        single_step_window = Window.partitionBy(\n",
    "            \"incoming_edge\", \n",
    "            \"outgoing_edge\", \n",
    "            \"current_cell\"\n",
    "        ).orderBy(\n",
    "            F.col(\"cost\").asc(), \n",
    "            F.col(\"via_edge\").asc() # Tie-breaker\n",
    "        )\n",
    "\n",
    "        # Select the single best path (Rank 1)\n",
    "        next_paths = all_paths.withColumn(\n",
    "            \"rnk\", \n",
    "            F.row_number().over(single_step_window)\n",
    "        ).filter(\n",
    "            F.col(\"rnk\") == 1\n",
    "        ).drop(\"rnk\")\n",
    "        \n",
    "        # CRITICAL CORRECTION: Group by all identifying keys (start, end, AND group)\n",
    "        #min_costs = all_paths.groupBy(\"incoming_edge\", \"outgoing_edge\", \"current_cell\").agg(\n",
    "        #    F.min(\"cost\").alias(\"min_cost\")\n",
    "        #).cache()\n",
    "\n",
    "        # Join back to filter to only the rows that match the minimum cost\n",
    "        #next_paths = all_paths.alias(\"T1\").join(\n",
    "        #    min_costs.alias(\"T2\"),\n",
    "            # CRITICAL CORRECTION: Use correct column names for join\n",
    "        #    [\n",
    "        #        F.col(\"T1.incoming_edge\") == F.col(\"T2.incoming_edge\"),\n",
    "        #        F.col(\"T1.outgoing_edge\") == F.col(\"T2.outgoing_edge\"),\n",
    "        #        F.col(\"T1.current_cell\") == F.col(\"T2.current_cell\"),\n",
    "        #        F.col(\"T1.cost\") == F.col(\"T2.min_cost\")\n",
    "        #    ],\n",
    "        #    \"inner\"\n",
    "        #).select(\n",
    "            # Use correct column names\n",
    "        #    F.col(\"T1.incoming_edge\"),\n",
    "        #    F.col(\"T1.outgoing_edge\"),\n",
    "        #    F.col(\"T2.min_cost\").alias(\"cost\"),\n",
    "        #    F.col(\"T1.via_edge\"),\n",
    "        #    F.col(\"T1.current_cell\"),\n",
    "        #    F.col(\"T1.is_converged\")\n",
    "        #).cache()\n",
    "        \n",
    "        # Unpersist DataFrames used only in this iteration\n",
    "        current_paths.unpersist()\n",
    "        new_paths.unpersist()\n",
    "        #min_costs.unpersist()\n",
    "\n",
    "        # --- 4. CONVERGENCE CHECK (The provided logic) ---\n",
    "        \n",
    "        # NOTE: Your logic requires 'update_convergence_status' to check if the path \n",
    "        # set is stable between iterations.\n",
    "        \n",
    "        # The update function is designed to check for changes to the full set of\n",
    "        # paths between two states. Here, we check the stability of the paths:\n",
    "        \n",
    "        # ..........\n",
    "        #next_paths = update_convergence_status(next_paths, current_paths)\n",
    "        # .........\n",
    "        \n",
    "        # Check if ALL rows in the resulting column are TRUE (meaning no changes occurred)\n",
    "        convergence_check_result = next_paths.select(\n",
    "            F.min(F.col(\"is_converged\")).alias(\"all_converged\")\n",
    "        ).collect()[0][\"all_converged\"]\n",
    "        \n",
    "        if convergence_check_result:\n",
    "            print(f\"âœ… {i} - Graph fully converged. Exiting shortest path calculation.\")\n",
    "            #break\n",
    "            \n",
    "        current_paths = next_paths\n",
    "    return current_paths.drop(\"is_converged\", \"current_cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d79b9",
   "metadata": {},
   "source": [
    "# 8- merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "91d51537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_shortcuts_to_main_table(\n",
    "    main_df: DataFrame,\n",
    "    new_shortcuts: DataFrame,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Merges newly computed shortcuts back into the main table.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Remove old shortcuts for the processed partitions\n",
    "    2. Add new shortcuts\n",
    "    \n",
    "    Args:\n",
    "        main_df: Main shortcut table\n",
    "        new_shortcuts: Newly computed shortcuts\n",
    "        partition_columns: Columns used for partitioning\n",
    "    \n",
    "    Returns:\n",
    "        Updated DataFrame with new shortcuts\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove old shortcuts for these partitions using left_anti join\n",
    "    #\n",
    "    # insteed of using this join function, we can just use the part of main_df that filtered out\n",
    "    main_df = main_df.select(\"incoming_edge\", \"outgoing_edge\", \"cost\", \"via_edge\")\n",
    "    new_shortcuts = new_shortcuts.select(\"incoming_edge\", \"outgoing_edge\", \"cost\", \"via_edge\")\n",
    "\n",
    "    remaining_df = main_df.alias(\"main\").join(\n",
    "        new_shortcuts.alias(\"update\"),\n",
    "        on=(\n",
    "            (F.col(\"main.incoming_edge\") == F.col(\"update.incoming_edge\")) &\n",
    "            (F.col(\"main.outgoing_edge\") == F.col(\"update.outgoing_edge\"))\n",
    "        ),\n",
    "        how=\"left_anti\"\n",
    "    )    \n",
    "    \n",
    "    # Combine with new shortcuts\n",
    "    updated_df = remaining_df.unionByName(new_shortcuts)\n",
    "    \n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "514f7c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/18 15:40:49 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "spark = initialize_spark()\n",
    "edges_df = read_edges(spark, file_path=\"data/burnaby_driving_simplified_edges_with_h3.csv\").cache()\n",
    "edges_cost_df = update_dummy_costs_for_edges(spark, file_path=\"data/burnaby_driving_simplified_edges_with_h3.csv\", edges_df=edges_df)\n",
    "shortcuts_df = initial_shortcuts_table(spark, file_path=\"data/burnaby_driving_edge_graph.csv\", edges_cost_df=edges_cost_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b05e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------------+-------+---------------+-------+\n",
      "|       incoming_edge|       outgoing_edge|            via_edge|              cost|lca_res|       via_cell|via_res|\n",
      "+--------------------+--------------------+--------------------+------------------+-------+---------------+-------+\n",
      "|(8703571310, 8703...|(8703571308, 8703...|(8703571308, 8703...|1.7882000000000002|     10|8f28de881760406|     15|\n",
      "|(7314138295, 6092...|(6092498237, 7314...|(6092498237, 7314...|             0.992|     11|8f28de8886c3b50|     15|\n",
      "|(9068804417, 9068...|(9068804425, 9463...|(9068804425, 9463...|3.4905999999999997|     11|8f28de8982e6759|     15|\n",
      "|(9289423408, 9289...|(9289423412, 9289...|(9289423412, 9289...|            2.6675|     11|8f28de1228c9309|     15|\n",
      "|(416104062, 34741...|(347415013, 25364...|(347415013, 25364...|1.9208666666666665|      9|8f28de898932612|     15|\n",
      "+--------------------+--------------------+--------------------+------------------+-------+---------------+-------+\n",
      "only showing top 5 rows\n",
      "number of shortcut in resolution 14 : 97963\n"
     ]
    }
   ],
   "source": [
    "for current_resolution in range(14,13,-1):\n",
    "    shortcuts_df = add_info_for_shortcuts(spark, shortcuts_df, edges_df) \n",
    "    shortcuts_df = shortcuts_df.checkpoint()\n",
    "    shortcuts_df.show(5)\n",
    "    shortcuts_df_filtered =filter_shortcuts_by_resolution(shortcuts_df,current_res=current_resolution)\n",
    "    shortcuts_df_with_cell =add_parent_cell_at_resolution(shortcuts_df_filtered, current_resolution)\n",
    "    \n",
    "    shortcuts_df_new = run_grouped_shortest_path_with_convergence( shortcuts_df_with_cell)\n",
    "    #shortcuts_df = merge_shortcuts_to_main_table(shortcuts_df, shortcuts_df_new)\n",
    "    print(f\"number of shortcut in resolution {current_resolution} : {shortcuts_df_new.count()}\")\n",
    "    #shortcuts_df_new.count()\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "b65d948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d58b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = shortcuts_df_new.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
