{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7295e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-21-openjdk-amd64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46945e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/17 00:06:10 WARN Utils: Your hostname, Bamdad-Beast, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/11/17 00:06:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/17 00:06:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PANDAS-OPTIMIZED SHORTEST PATH CALCULATOR\n",
      "============================================================\n",
      "Initial shortcuts:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  A|  C| 5.0|        C|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "ITERATION 1\n",
      "============================================================\n",
      "\n",
      "Filtered shortcuts (cost < 10):\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  A|  C| 5.0|        C|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "New shortcuts computed:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "Updated shortcuts table:\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "Total shortcuts: 9\n",
      "\n",
      "============================================================\n",
      "ITERATION 2\n",
      "============================================================\n",
      "\n",
      "Filtered shortcuts (cost < 10):\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "New shortcuts computed:\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "Updated shortcuts table:\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "Total shortcuts: 9\n",
      "\n",
      "============================================================\n",
      "ITERATION 3\n",
      "============================================================\n",
      "\n",
      "Filtered shortcuts (cost < 10):\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "New shortcuts computed:\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "Updated shortcuts table:\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "Total shortcuts: 9\n",
      "\n",
      "============================================================\n",
      "FINAL RESULT\n",
      "============================================================\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "BENCHMARK\n",
      "============================================================\n",
      "Input: 70 edges\n",
      "\n",
      "Result: 190 shortest paths\n",
      "Time taken: 0.51 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pandas-optimized implementation for all-pairs shortest path calculation.\n",
    "Best for: Small to medium groups (10-500 rows per partition).\n",
    "Uses vectorized pandas operations within each partition via applyInPandas.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "\n",
    "def compute_shortest_paths_per_partition(\n",
    "    df_shortcuts: DataFrame,\n",
    "    partition_columns: list,\n",
    "    max_iterations_per_group: int = None\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes all-pairs shortest paths using vectorized pandas operations.\n",
    "    Uses Spark's applyInPandas to process each partition with optimized pandas code.\n",
    "    \n",
    "    This approach is significantly faster than pure Spark SQL for small groups\n",
    "    because it avoids shuffle operations and uses pandas' C-optimized operations.\n",
    "    \n",
    "    Args:\n",
    "        df_shortcuts: DataFrame with schema (src, dst, cost, next_node, partition_col1, ...)\n",
    "        partition_columns: List of column names to group by\n",
    "        max_iterations_per_group: Maximum iterations per group (default: group size)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with computed shortest paths, preserving partition columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Infer the schema for output\n",
    "    # We need to know the data types of partition columns\n",
    "    partition_fields = [\n",
    "        StructField(col, df_shortcuts.schema[col].dataType, False) \n",
    "        for col in partition_columns\n",
    "    ]\n",
    "    \n",
    "    output_schema = StructType([\n",
    "        StructField(\"src\", StringType(), False),\n",
    "        StructField(\"dst\", StringType(), False),\n",
    "        StructField(\"cost\", DoubleType(), False),\n",
    "        StructField(\"next_node\", StringType(), False)\n",
    "    ] + partition_fields)\n",
    "    \n",
    "    def process_partition_pandas(pdf):\n",
    "        \"\"\"\n",
    "        Process a single partition using vectorized pandas operations.\n",
    "        This function runs inside each Spark executor on the partition's data.\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        \n",
    "        if len(pdf) == 0:\n",
    "            return pd.DataFrame(columns=['src', 'dst', 'cost', 'next_node'] + partition_columns)\n",
    "        \n",
    "        # Store partition values (all rows have same partition values)\n",
    "        partition_values = {col: pdf[col].iloc[0] for col in partition_columns}\n",
    "        \n",
    "        # Start with the input edges as initial paths\n",
    "        paths = pdf[['src', 'dst', 'cost', 'next_node']].copy()\n",
    "        \n",
    "        # Remove duplicates, keeping minimum cost\n",
    "        # No need to remove duplicates\n",
    "        paths = paths.loc[paths.groupby(['src', 'dst'])['cost'].idxmin()].reset_index(drop=True)\n",
    "        \n",
    "        # Set max iterations\n",
    "        max_iters = max_iterations_per_group if max_iterations_per_group else len(pdf)\n",
    "        \n",
    "        for iteration in range(max_iters):\n",
    "            # Vectorized merge: if A->B and B->C exist, create A->C\n",
    "            # This is equivalent to the self-join in Spark but uses pandas merge\n",
    "            new_paths = paths.merge(\n",
    "                paths,\n",
    "                left_on='dst',\n",
    "                right_on='src',\n",
    "                suffixes=('_L', '_R')\n",
    "            )\n",
    "            \n",
    "            # Calculate new costs and keep the first hop's next_node\n",
    "            new_paths = new_paths[['src_L', 'dst_R', 'cost_L', 'cost_R', 'next_node_L']]\n",
    "            new_paths['cost'] = new_paths['cost_L'] + new_paths['cost_R']\n",
    "            new_paths = new_paths.rename(columns={\n",
    "                'src_L': 'src',\n",
    "                'dst_R': 'dst',\n",
    "                'next_node_L': 'next_node'\n",
    "            })[['src', 'dst', 'cost', 'next_node']]\n",
    "            \n",
    "            # Filter out self-loops\n",
    "            new_paths = new_paths[new_paths['src'] != new_paths['dst']]\n",
    "            \n",
    "            if len(new_paths) == 0:\n",
    "                break  # No new paths to add\n",
    "            \n",
    "            # Combine existing and new paths\n",
    "            combined = pd.concat([paths, new_paths], ignore_index=True)\n",
    "            \n",
    "            # Keep only minimum cost for each (src, dst) pair - vectorized operation\n",
    "            updated_paths = combined.loc[\n",
    "                combined.groupby(['src', 'dst'])['cost'].idxmin()\n",
    "            ].reset_index(drop=True)\n",
    "            \n",
    "            # Check convergence: if no change in paths, we're done\n",
    "            if len(updated_paths) == len(paths):\n",
    "                # Quick check: if sizes match, check if content is identical\n",
    "                merged = updated_paths.merge(\n",
    "                    paths, \n",
    "                    on=['src', 'dst', 'cost', 'next_node']\n",
    "                )\n",
    "                if len(merged) == len(paths):\n",
    "                    break  # Converged\n",
    "            \n",
    "            paths = updated_paths\n",
    "        \n",
    "        # Add partition columns back to result\n",
    "        for col, val in partition_values.items():\n",
    "            paths[col] = val\n",
    "        \n",
    "        return paths\n",
    "    \n",
    "    # Apply the pandas function to each partition group\n",
    "    result = df_shortcuts.groupBy(partition_columns).applyInPandas(\n",
    "        process_partition_pandas,\n",
    "        schema=output_schema\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def merge_shortcuts_to_main_table(\n",
    "    main_df: DataFrame,\n",
    "    new_shortcuts: DataFrame,\n",
    "    partition_columns: list\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Merges newly computed shortcuts back into the main table.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Remove old shortcuts for the processed partitions\n",
    "    2. Add new shortcuts\n",
    "    \n",
    "    Args:\n",
    "        main_df: Main shortcut table\n",
    "        new_shortcuts: Newly computed shortcuts\n",
    "        partition_columns: Columns used for partitioning\n",
    "    \n",
    "    Returns:\n",
    "        Updated DataFrame with new shortcuts\n",
    "    \"\"\"\n",
    "    # Get the partition values that were recomputed\n",
    "    recomputed_partitions = new_shortcuts.select(partition_columns).distinct()\n",
    "    \n",
    "    # Remove old shortcuts for these partitions using left_anti join\n",
    "    #\n",
    "    # insteed of using this join function, we can just use the part of main_df that filtered out\n",
    "    remaining_df = main_df.alias(\"main\").join(\n",
    "        recomputed_partitions.alias(\"recomp\"),\n",
    "        [F.col(f\"main.{col}\") == F.col(f\"recomp.{col}\") for col in partition_columns],\n",
    "        \"left_anti\"\n",
    "    )\n",
    "    \n",
    "    # Combine with new shortcuts\n",
    "    updated_df = remaining_df.unionByName(new_shortcuts)\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "\n",
    "def example_pipeline(spark: SparkSession):\n",
    "    \"\"\"\n",
    "    Example usage in an iterative pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example: Create initial shortcuts table\n",
    "    data = [\n",
    "        (\"A\", \"B\", 1.0, \"B\", \"region1\"),\n",
    "        (\"B\", \"C\", 2.0, \"C\", \"region1\"),\n",
    "        (\"C\", \"D\", 1.0, \"D\", \"region1\"),\n",
    "        (\"A\", \"C\", 5.0, \"C\", \"region1\"),  # Suboptimal shortcut\n",
    "        (\"E\", \"F\", 1.0, \"F\", \"region2\"),\n",
    "        (\"F\", \"G\", 2.0, \"G\", \"region2\"),\n",
    "    ]\n",
    "    \n",
    "    shortcuts = spark.createDataFrame(\n",
    "        data, \n",
    "        [\"src\", \"dst\", \"cost\", \"next_node\", \"region_id\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Initial shortcuts:\")\n",
    "    shortcuts.show()\n",
    "    \n",
    "    # Iterative pipeline\n",
    "    for iteration in range(3):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ITERATION {iteration + 1}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Step 1: Filter (example: keep only cost < 10)\n",
    "        filtered = shortcuts.filter(F.col(\"cost\") < 10)\n",
    "        \n",
    "        print(f\"\\nFiltered shortcuts (cost < 10):\")\n",
    "        filtered.show()\n",
    "        \n",
    "        # Step 2: Compute shortest paths per partition using pandas\n",
    "        new_shortcuts = compute_shortest_paths_per_partition(\n",
    "            filtered,\n",
    "            partition_columns=[\"region_id\"],\n",
    "            max_iterations_per_group=100\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nNew shortcuts computed:\")\n",
    "        new_shortcuts.show()\n",
    "        \n",
    "        # Step 3: Merge back to main table\n",
    "        shortcuts = merge_shortcuts_to_main_table(\n",
    "            shortcuts,\n",
    "            new_shortcuts,\n",
    "            partition_columns=[\"region_id\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nUpdated shortcuts table:\")\n",
    "        shortcuts.show()\n",
    "        \n",
    "        # Check if converged (optional)\n",
    "        shortcut_count = shortcuts.count()\n",
    "        print(f\"Total shortcuts: {shortcut_count}\")\n",
    "    \n",
    "    return shortcuts\n",
    "\n",
    "\n",
    "def benchmark_comparison(spark: SparkSession):\n",
    "    \"\"\"\n",
    "    Simple benchmark to compare convergence speed.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Create a slightly larger example\n",
    "    data = []\n",
    "    for i in range(20):\n",
    "        for j in range(i+1, min(i+5, 20)):\n",
    "            data.append((f\"N{i}\", f\"N{j}\", float(j-i), f\"N{j}\", \"partition1\"))\n",
    "    \n",
    "    df = spark.createDataFrame(data, [\"src\", \"dst\", \"cost\", \"next_node\", \"region_id\"])\n",
    "    \n",
    "    print(f\"Input: {df.count()} edges\")\n",
    "    \n",
    "    start = time.time()\n",
    "    result = compute_shortest_paths_per_partition(\n",
    "        df,\n",
    "        partition_columns=[\"region_id\"],\n",
    "        max_iterations_per_group=50\n",
    "    )\n",
    "    result.cache()\n",
    "    result_count = result.count()\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f\"\\nResult: {result_count} shortest paths\")\n",
    "    print(f\"Time taken: {end - start:.2f} seconds\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PandasShortestPath\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"PANDAS-OPTIMIZED SHORTEST PATH CALCULATOR\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Run example pipeline\n",
    "    result = example_pipeline(spark)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULT\")\n",
    "    print(\"=\"*60)\n",
    "    result.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BENCHMARK\")\n",
    "    print(\"=\"*60)\n",
    "    benchmark_result = benchmark_comparison(spark)\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e717350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converging_check(old_paths: DataFrame, new_paths: DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if two DataFrames of paths are identical.\n",
    "    Used to determine convergence in the shortest path algorithm.\n",
    "    \"\"\" \n",
    "    # Join on all columns to see if they match\n",
    "    joined = new_paths.join(\n",
    "        old_paths,\n",
    "        on=[\"incoming_edge\",\"outgoing_edge\",\"cost\"],\n",
    "        how=\"anti_left\"\n",
    "    )\n",
    "    return (joined.limit(1).count()==0)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
