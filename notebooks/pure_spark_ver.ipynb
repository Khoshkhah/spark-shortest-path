{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e6fd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/16 16:52:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-21-openjdk-amd64'\n",
    "# Make sure checkpoint directory exists\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "from pyspark.sql import SparkSession\n",
    "# Then run\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PureSparkShortestPath\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setCheckpointDir(\"checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "359802d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/16 16:52:59 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shortcuts:\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  A|  C| 5.0|        C|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "ITERATION 1\n",
      "============================================================\n",
      "Processing 2 partitions...\n",
      "Processing partition 1/2: region1\n",
      "Processing partition 2/2: region2\n",
      "\n",
      "New shortcuts computed:\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "Updated shortcuts table:\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "ITERATION 2\n",
      "============================================================\n",
      "Processing 2 partitions...\n",
      "Processing partition 1/2: region1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing partition 2/2: region2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New shortcuts computed:\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "Updated shortcuts table:\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1967.showString.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3482)\n\tat java.base/java.util.ArrayList.grow(ArrayList.java:237)\n\tat java.base/java.util.ArrayList.grow(ArrayList.java:244)\n\tat java.base/java.util.ArrayList.add(ArrayList.java:483)\n\tat java.base/java.util.ArrayList.add(ArrayList.java:496)\n\tat org.apache.spark.sql.catalyst.util.StringConcat.append(StringUtils.scala:47)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$explainString$1(QueryExecution.scala:312)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$explainString$1$adapted(QueryExecution.scala:312)\n\tat org.apache.spark.sql.execution.QueryExecution$$Lambda/0x0000746af0bf72c0.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$generateTreeString$2(TreeNode.scala:1032)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$Lambda/0x0000746af0cf73f8.apply$mcVI$sp(Unknown Source)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1030)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$generateTreeString$4(TreeNode.scala:1071)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$generateTreeString$4$adapted(TreeNode.scala:1069)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$Lambda/0x0000746af0cf7028.apply(Unknown Source)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1069)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 269\u001b[39m\n\u001b[32m    262\u001b[39m spark = SparkSession.builder \\\n\u001b[32m    263\u001b[39m     .appName(\u001b[33m\"\u001b[39m\u001b[33mPureSparkShortestPath\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m    264\u001b[39m     .config(\u001b[33m\"\u001b[39m\u001b[33mspark.driver.memory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m4g\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m    265\u001b[39m     .getOrCreate()\n\u001b[32m    267\u001b[39m spark.sparkContext.setCheckpointDir(\u001b[33m\"\u001b[39m\u001b[33mcheckpoints\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m result = \u001b[43mexample_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m    272\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFINAL RESULT\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 256\u001b[39m, in \u001b[36mexample_pipeline\u001b[39m\u001b[34m(spark)\u001b[39m\n\u001b[32m    249\u001b[39m     shortcuts = merge_shortcuts_to_main_table(\n\u001b[32m    250\u001b[39m         shortcuts,\n\u001b[32m    251\u001b[39m         new_shortcuts,\n\u001b[32m    252\u001b[39m         partition_columns=[\u001b[33m\"\u001b[39m\u001b[33mregion_id\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    253\u001b[39m     )\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUpdated shortcuts table:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[43mshortcuts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m shortcuts\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:285\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:303\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    298\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    299\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1967.showString.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3482)\n\tat java.base/java.util.ArrayList.grow(ArrayList.java:237)\n\tat java.base/java.util.ArrayList.grow(ArrayList.java:244)\n\tat java.base/java.util.ArrayList.add(ArrayList.java:483)\n\tat java.base/java.util.ArrayList.add(ArrayList.java:496)\n\tat org.apache.spark.sql.catalyst.util.StringConcat.append(StringUtils.scala:47)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$explainString$1(QueryExecution.scala:312)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$explainString$1$adapted(QueryExecution.scala:312)\n\tat org.apache.spark.sql.execution.QueryExecution$$Lambda/0x0000746af0bf72c0.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$generateTreeString$2(TreeNode.scala:1032)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$Lambda/0x0000746af0cf73f8.apply$mcVI$sp(Unknown Source)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1030)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$generateTreeString$4(TreeNode.scala:1071)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$generateTreeString$4$adapted(TreeNode.scala:1069)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$Lambda/0x0000746af0cf7028.apply(Unknown Source)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1069)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:1078)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pure Spark SQL implementation for all-pairs shortest path calculation.\n",
    "Best for: Large groups (500+ rows per partition) or when pandas is unavailable.\n",
    "Uses distributed Spark operations (joins, aggregations) throughout.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def compute_shortest_paths_for_group(df_group: DataFrame, max_iterations: int = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes all-pairs shortest paths for a single group using pure Spark SQL.\n",
    "    Uses iterative path extension with self-joins.\n",
    "    \n",
    "    Input schema: (src, dst, cost, next_node)\n",
    "    Output schema: (src, dst, cost, next_node)\n",
    "    \n",
    "    Args:\n",
    "        df_group: DataFrame containing edges/shortcuts for a single group\n",
    "        max_iterations: Maximum iterations (default: row count of group)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with shortest paths (all pairs reachable within the group)\n",
    "    \"\"\"\n",
    "    if max_iterations is None:\n",
    "        max_iterations = df_group.count()\n",
    "    \n",
    "    # Rename to standard internal names\n",
    "    current_paths = df_group.select(\n",
    "        F.col(\"src\").alias(\"path_start\"),\n",
    "        F.col(\"dst\").alias(\"path_end\"),\n",
    "        F.col(\"cost\"),\n",
    "        F.col(\"next_node\")\n",
    "    ).cache()\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        \n",
    "        # Extend paths by joining: if we have A->B and B->C, create A->C\n",
    "        new_paths = current_paths.alias(\"L\").join(\n",
    "            current_paths.alias(\"R\"),\n",
    "            F.col(\"L.path_end\") == F.col(\"R.path_start\"),\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            F.col(\"L.path_start\").alias(\"path_start\"),\n",
    "            F.col(\"R.path_end\").alias(\"path_end\"),\n",
    "            (F.col(\"L.cost\") + F.col(\"R.cost\")).alias(\"cost\"),\n",
    "            F.col(\"L.next_node\").alias(\"next_node\")  # Keep first hop\n",
    "        ).filter(F.col(\"path_start\") != F.col(\"path_end\"))  # No self-loops\n",
    "        \n",
    "        # Combine with existing paths\n",
    "        all_paths = current_paths.unionByName(new_paths)\n",
    "        \n",
    "        # Keep only minimum cost path for each (start, end) pair\n",
    "        min_costs = all_paths.groupBy(\"path_start\", \"path_end\").agg(\n",
    "            F.min(\"cost\").alias(\"min_cost\")\n",
    "        )\n",
    "        \n",
    "        next_paths = all_paths.alias(\"T1\").join(\n",
    "            min_costs.alias(\"T2\"),\n",
    "            (F.col(\"T1.path_start\") == F.col(\"T2.path_start\")) &\n",
    "            (F.col(\"T1.path_end\") == F.col(\"T2.path_end\")) &\n",
    "            (F.col(\"T1.cost\") == F.col(\"T2.min_cost\")),\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            F.col(\"T1.path_start\"),\n",
    "            F.col(\"T1.path_end\"),\n",
    "            F.col(\"T2.min_cost\").alias(\"cost\"),\n",
    "            F.col(\"T1.next_node\")\n",
    "        ).dropDuplicates([\"path_start\", \"path_end\"]).cache()\n",
    "        \n",
    "        # Check convergence\n",
    "        if next_paths.count() == current_paths.count():\n",
    "            diff = current_paths.subtract(next_paths).count()\n",
    "            if diff == 0:\n",
    "                current_paths.unpersist()\n",
    "                # Rename back to original schema\n",
    "                return next_paths.select(\n",
    "                    F.col(\"path_start\").alias(\"src\"),\n",
    "                    F.col(\"path_end\").alias(\"dst\"),\n",
    "                    F.col(\"cost\"),\n",
    "                    F.col(\"next_node\")\n",
    "                )\n",
    "        \n",
    "        current_paths.unpersist()\n",
    "        current_paths = next_paths\n",
    "    \n",
    "    # Return with original column names\n",
    "    return current_paths.select(\n",
    "        F.col(\"path_start\").alias(\"src\"),\n",
    "        F.col(\"path_end\").alias(\"dst\"),\n",
    "        F.col(\"cost\"),\n",
    "        F.col(\"next_node\")\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_shortest_paths_per_partition(\n",
    "    df_shortcuts: DataFrame,\n",
    "    partition_columns: list,\n",
    "    max_iterations_per_group: int = None,\n",
    "    checkpoint_interval: int = 5\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes all-pairs shortest paths for each partition using pure Spark SQL.\n",
    "    Processes each partition separately to avoid cross-partition joins.\n",
    "    \n",
    "    Args:\n",
    "        df_shortcuts: DataFrame with schema (src, dst, cost, next_node, partition_col1, ...)\n",
    "        partition_columns: List of column names to group by\n",
    "        max_iterations_per_group: Maximum iterations per group\n",
    "        checkpoint_interval: Checkpoint every N partitions to prevent lineage issues\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with computed shortest paths, preserving partition columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add a unique partition ID for easier tracking\n",
    "    df_with_pid = df_shortcuts.withColumn(\n",
    "        \"_partition_id\",\n",
    "        F.concat_ws(\"_\", *[F.col(c).cast(\"string\") for c in partition_columns])\n",
    "    )\n",
    "    \n",
    "    # Get all unique partition IDs\n",
    "    partition_ids = [row[\"_partition_id\"] for row in \n",
    "                     df_with_pid.select(\"_partition_id\").distinct().collect()]\n",
    "    \n",
    "    print(f\"Processing {len(partition_ids)} partitions...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, pid in enumerate(partition_ids):\n",
    "        print(f\"Processing partition {idx + 1}/{len(partition_ids)}: {pid}\")\n",
    "        \n",
    "        # Extract data for this partition only\n",
    "        partition_df = df_with_pid.filter(F.col(\"_partition_id\") == pid)\n",
    "        \n",
    "        # Get partition column values (for adding back later)\n",
    "        partition_values = partition_df.select(partition_columns).first().asDict()\n",
    "        \n",
    "        # Compute shortest paths for this partition\n",
    "        shortest_paths = compute_shortest_paths_for_group(\n",
    "            partition_df.select(\"src\", \"dst\", \"cost\", \"next_node\"),\n",
    "            max_iterations=max_iterations_per_group\n",
    "        )\n",
    "        \n",
    "        # Add back partition columns\n",
    "        for col, val in partition_values.items():\n",
    "            shortest_paths = shortest_paths.withColumn(col, F.lit(val))\n",
    "        \n",
    "        results.append(shortest_paths)\n",
    "        \n",
    "        # Checkpoint periodically to prevent long lineage\n",
    "        if checkpoint_interval and (idx + 1) % checkpoint_interval == 0:\n",
    "            print(f\"Checkpointing at partition {idx + 1}...\")\n",
    "    \n",
    "    # Union all results\n",
    "    if not results:\n",
    "        return df_shortcuts.limit(0)  # Empty DataFrame with same schema\n",
    "    \n",
    "    final_result = results[0]\n",
    "    for result_df in results[1:]:\n",
    "        final_result = final_result.unionByName(result_df)\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "\n",
    "def merge_shortcuts_to_main_table(\n",
    "    main_df: DataFrame,\n",
    "    new_shortcuts: DataFrame,\n",
    "    partition_columns: list\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Merges newly computed shortcuts back into the main table.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Remove old shortcuts for the processed partitions\n",
    "    2. Add new shortcuts\n",
    "    \n",
    "    Args:\n",
    "        main_df: Main shortcut table\n",
    "        new_shortcuts: Newly computed shortcuts\n",
    "        partition_columns: Columns used for partitioning\n",
    "    \n",
    "    Returns:\n",
    "        Updated DataFrame with new shortcuts\n",
    "    \"\"\"\n",
    "    # Get the partition values that were recomputed\n",
    "    recomputed_partitions = new_shortcuts.select(partition_columns).distinct()\n",
    "    \n",
    "    # Remove old shortcuts for these partitions using left_anti join\n",
    "    remaining_df = main_df.alias(\"main\").join(\n",
    "        recomputed_partitions.alias(\"recomp\"),\n",
    "        [F.col(f\"main.{col}\") == F.col(f\"recomp.{col}\") for col in partition_columns],\n",
    "        \"left_anti\"\n",
    "    )\n",
    "    \n",
    "    # Combine with new shortcuts\n",
    "    updated_df = remaining_df.unionByName(new_shortcuts)\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "\n",
    "def example_pipeline(spark: SparkSession):\n",
    "    \"\"\"\n",
    "    Example usage in an iterative pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example: Create initial shortcuts table\n",
    "    data = [\n",
    "        (\"A\", \"B\", 1.0, \"B\", \"region1\"),\n",
    "        (\"B\", \"C\", 2.0, \"C\", \"region1\"),\n",
    "        (\"C\", \"D\", 1.0, \"D\", \"region1\"),\n",
    "        (\"A\", \"C\", 5.0, \"C\", \"region1\"),  # Suboptimal shortcut\n",
    "        (\"E\", \"F\", 1.0, \"F\", \"region2\"),\n",
    "        (\"F\", \"G\", 2.0, \"G\", \"region2\"),\n",
    "    ]\n",
    "    \n",
    "    shortcuts = spark.createDataFrame(\n",
    "        data, \n",
    "        [\"src\", \"dst\", \"cost\", \"next_node\", \"region_id\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Initial shortcuts:\")\n",
    "    shortcuts.show()\n",
    "    \n",
    "    # Iterative pipeline\n",
    "    for iteration in range(3):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ITERATION {iteration + 1}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Step 1: Filter (example: keep only cost < 10)\n",
    "        filtered = shortcuts.filter(F.col(\"cost\") < 10)\n",
    "        \n",
    "        # Step 2: Compute shortest paths per partition\n",
    "        new_shortcuts = compute_shortest_paths_per_partition(\n",
    "            filtered,\n",
    "            partition_columns=[\"region_id\"],\n",
    "            max_iterations_per_group=100\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nNew shortcuts computed:\")\n",
    "        new_shortcuts.show()\n",
    "        \n",
    "        # Step 3: Merge back to main table\n",
    "        shortcuts = merge_shortcuts_to_main_table(\n",
    "            shortcuts,\n",
    "            new_shortcuts,\n",
    "            partition_columns=[\"region_id\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nUpdated shortcuts table:\")\n",
    "        shortcuts.show()\n",
    "    \n",
    "    return shortcuts\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PureSparkShortestPath\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    spark.sparkContext.setCheckpointDir(\"checkpoints\")\n",
    "    \n",
    "    result = example_pipeline(spark)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULT\")\n",
    "    print(\"=\"*60)\n",
    "    result.show()\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6420b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/16 16:54:50 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PURE SPARK SQL SHORTEST PATH CALCULATOR\n",
      "============================================================\n",
      "Initial shortcuts:\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  A|  C| 5.0|        C|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "ITERATION 1\n",
      "============================================================\n",
      "Processing 2 partitions...\n",
      "\n",
      "Processing partition 1/2: region1\n",
      "  Iteration 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpointed at iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converged at iteration 3\n",
      "\n",
      "Processing partition 2/2: region2\n",
      "  Iteration 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converged at iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining results from 2 partitions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New shortcuts computed:\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "Updated shortcuts table:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "ITERATION 2\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2 partitions...\n",
      "\n",
      "Processing partition 1/2: region1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converged at iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing partition 2/2: region2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converged at iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"RemoteBlock-temp-file-clean-thread\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle.allocateInstance(DirectMethodHandle.java:501)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.newInvokeSpecial(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.storage.BlockManager$RemoteBlockDownloadFileManager.org$apache$spark$storage$BlockManager$RemoteBlockDownloadFileManager$$keepCleaning(BlockManager.scala:2274)\n",
      "\tat org.apache.spark.storage.BlockManager$RemoteBlockDownloadFileManager$$anon$2.run(BlockManager.scala:2240)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2941.count.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:101)\n\tat java.base/java.lang.StringBuilder.<init>(StringBuilder.java:119)\n\tat org.apache.spark.sql.catalyst.util.StringConcat.toString(StringUtils.scala:63)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$PlanStringConcat.toString(StringUtils.scala:149)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:314)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.onUpdatePlan(AdaptiveSparkPlanExec.scala:858)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$2(AdaptiveSparkPlanExec.scala:292)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda/0x0000746af0f9c870.apply$mcVJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.scala:18)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$1(AdaptiveSparkPlanExec.scala:292)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda/0x0000746af0f8cd00.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:279)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:402)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$count$1(Dataset.scala:1500)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$count$1$adapted(Dataset.scala:1499)\n\tat org.apache.spark.sql.classic.Dataset$$Lambda/0x0000746af11043c8.apply(Unknown Source)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\n\tat org.apache.spark.sql.classic.Dataset$$Lambda/0x0000746af0d05d40.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset$$Lambda/0x0000746af0bebb58.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000746af0bf5748.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000746af0bf4b30.apply(Unknown Source)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager$$Lambda/0x0000746af0bf4df8.apply(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 305\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPURE SPARK SQL SHORTEST PATH CALCULATOR\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    303\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m result = \u001b[43mexample_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m    308\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFINAL RESULT\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 267\u001b[39m, in \u001b[36mexample_pipeline\u001b[39m\u001b[34m(spark)\u001b[39m\n\u001b[32m    264\u001b[39m filtered = shortcuts.filter(F.col(\u001b[33m\"\u001b[39m\u001b[33mcost\u001b[39m\u001b[33m\"\u001b[39m) < \u001b[32m10\u001b[39m)\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m# Step 2: Compute shortest paths per partition\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m new_shortcuts = \u001b[43mcompute_shortest_paths_per_partition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mregion_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iterations_per_group\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m    271\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNew shortcuts computed:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    274\u001b[39m new_shortcuts.show()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 174\u001b[39m, in \u001b[36mcompute_shortest_paths_per_partition\u001b[39m\u001b[34m(df_shortcuts, partition_columns, max_iterations_per_group)\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# Materialize this partition's results\u001b[39;00m\n\u001b[32m    173\u001b[39m     shortest_paths = shortest_paths.persist()\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[43mshortest_paths\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Force materialization\u001b[39;00m\n\u001b[32m    176\u001b[39m     results.append(shortest_paths)\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# Union all results\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:439\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o2941.count.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:101)\n\tat java.base/java.lang.StringBuilder.<init>(StringBuilder.java:119)\n\tat org.apache.spark.sql.catalyst.util.StringConcat.toString(StringUtils.scala:63)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$PlanStringConcat.toString(StringUtils.scala:149)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:314)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.onUpdatePlan(AdaptiveSparkPlanExec.scala:858)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$2(AdaptiveSparkPlanExec.scala:292)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda/0x0000746af0f9c870.apply$mcVJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.scala:18)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$1(AdaptiveSparkPlanExec.scala:292)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda/0x0000746af0f8cd00.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:279)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:402)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$count$1(Dataset.scala:1500)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$count$1$adapted(Dataset.scala:1499)\n\tat org.apache.spark.sql.classic.Dataset$$Lambda/0x0000746af11043c8.apply(Unknown Source)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\n\tat org.apache.spark.sql.classic.Dataset$$Lambda/0x0000746af0d05d40.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset$$Lambda/0x0000746af0bebb58.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000746af0bf5748.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000746af0bf4b30.apply(Unknown Source)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager$$Lambda/0x0000746af0bf4df8.apply(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pure Spark SQL implementation for all-pairs shortest path calculation.\n",
    "Best for: Large groups (500+ rows per partition) or when pandas is unavailable.\n",
    "Uses distributed Spark operations (joins, aggregations) throughout.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def compute_shortest_paths_for_group(df_group: DataFrame, max_iterations: int = None, \n",
    "                                      checkpoint_interval: int = 3) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes all-pairs shortest paths for a single group using pure Spark SQL.\n",
    "    Uses iterative path extension with self-joins.\n",
    "    \n",
    "    Input schema: (src, dst, cost, next_node)\n",
    "    Output schema: (src, dst, cost, next_node)\n",
    "    \n",
    "    Args:\n",
    "        df_group: DataFrame containing edges/shortcuts for a single group\n",
    "        max_iterations: Maximum iterations (default: row count of group)\n",
    "        checkpoint_interval: Checkpoint every N iterations to prevent lineage explosion\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with shortest paths (all pairs reachable within the group)\n",
    "    \"\"\"\n",
    "    if max_iterations is None:\n",
    "        max_iterations = df_group.count()\n",
    "    \n",
    "    # Rename to standard internal names\n",
    "    current_paths = df_group.select(\n",
    "        F.col(\"src\").alias(\"path_start\"),\n",
    "        F.col(\"dst\").alias(\"path_end\"),\n",
    "        F.col(\"cost\"),\n",
    "        F.col(\"next_node\")\n",
    "    )\n",
    "    \n",
    "    # Materialize initial data\n",
    "    current_paths = current_paths.persist()\n",
    "    current_paths.count()  # Force materialization\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        print(f\"  Iteration {iteration}...\")\n",
    "        \n",
    "        # Extend paths by joining: if we have A->B and B->C, create A->C\n",
    "        new_paths = current_paths.alias(\"L\").join(\n",
    "            current_paths.alias(\"R\"),\n",
    "            F.col(\"L.path_end\") == F.col(\"R.path_start\"),\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            F.col(\"L.path_start\").alias(\"path_start\"),\n",
    "            F.col(\"R.path_end\").alias(\"path_end\"),\n",
    "            (F.col(\"L.cost\") + F.col(\"R.cost\")).alias(\"cost\"),\n",
    "            F.col(\"L.next_node\").alias(\"next_node\")  # Keep first hop\n",
    "        ).filter(F.col(\"path_start\") != F.col(\"path_end\"))  # No self-loops\n",
    "        \n",
    "        # Combine with existing paths\n",
    "        all_paths = current_paths.unionByName(new_paths)\n",
    "        \n",
    "        # Keep only minimum cost path for each (start, end) pair\n",
    "        min_costs = all_paths.groupBy(\"path_start\", \"path_end\").agg(\n",
    "            F.min(\"cost\").alias(\"min_cost\")\n",
    "        )\n",
    "        \n",
    "        next_paths = all_paths.alias(\"T1\").join(\n",
    "            min_costs.alias(\"T2\"),\n",
    "            (F.col(\"T1.path_start\") == F.col(\"T2.path_start\")) &\n",
    "            (F.col(\"T1.path_end\") == F.col(\"T2.path_end\")) &\n",
    "            (F.col(\"T1.cost\") == F.col(\"T2.min_cost\")),\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            F.col(\"T1.path_start\"),\n",
    "            F.col(\"T1.path_end\"),\n",
    "            F.col(\"T2.min_cost\").alias(\"cost\"),\n",
    "            F.col(\"T1.next_node\")\n",
    "        ).dropDuplicates([\"path_start\", \"path_end\"])\n",
    "        \n",
    "        # Checkpoint periodically to break lineage\n",
    "        if iteration % checkpoint_interval == 0:\n",
    "            next_paths = next_paths.checkpoint()\n",
    "            print(f\"  Checkpointed at iteration {iteration}\")\n",
    "        \n",
    "        # Materialize and cache\n",
    "        next_paths = next_paths.persist()\n",
    "        next_count = next_paths.count()  # Force materialization\n",
    "        \n",
    "        # Check convergence\n",
    "        current_count = current_paths.count()\n",
    "        \n",
    "        if next_count == current_count:\n",
    "            diff = current_paths.subtract(next_paths).count()\n",
    "            if diff == 0:\n",
    "                print(f\"  Converged at iteration {iteration}\")\n",
    "                current_paths.unpersist()\n",
    "                # Rename back to original schema\n",
    "                return next_paths.select(\n",
    "                    F.col(\"path_start\").alias(\"src\"),\n",
    "                    F.col(\"path_end\").alias(\"dst\"),\n",
    "                    F.col(\"cost\"),\n",
    "                    F.col(\"next_node\")\n",
    "                )\n",
    "        \n",
    "        # Unpersist old data and move to next iteration\n",
    "        current_paths.unpersist()\n",
    "        current_paths = next_paths\n",
    "    \n",
    "    # Return with original column names\n",
    "    return current_paths.select(\n",
    "        F.col(\"path_start\").alias(\"src\"),\n",
    "        F.col(\"path_end\").alias(\"dst\"),\n",
    "        F.col(\"cost\"),\n",
    "        F.col(\"next_node\")\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_shortest_paths_per_partition(\n",
    "    df_shortcuts: DataFrame,\n",
    "    partition_columns: list,\n",
    "    max_iterations_per_group: int = None\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes all-pairs shortest paths for each partition using pure Spark SQL.\n",
    "    Processes each partition separately to avoid cross-partition joins.\n",
    "    \n",
    "    Args:\n",
    "        df_shortcuts: DataFrame with schema (src, dst, cost, next_node, partition_col1, ...)\n",
    "        partition_columns: List of column names to group by\n",
    "        max_iterations_per_group: Maximum iterations per group\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with computed shortest paths, preserving partition columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add a unique partition ID for easier tracking\n",
    "    df_with_pid = df_shortcuts.withColumn(\n",
    "        \"_partition_id\",\n",
    "        F.concat_ws(\"_\", *[F.col(c).cast(\"string\") for c in partition_columns])\n",
    "    )\n",
    "    \n",
    "    # Get all unique partition IDs\n",
    "    partition_ids = [row[\"_partition_id\"] for row in \n",
    "                     df_with_pid.select(\"_partition_id\").distinct().collect()]\n",
    "    \n",
    "    print(f\"Processing {len(partition_ids)} partitions...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, pid in enumerate(partition_ids):\n",
    "        print(f\"\\nProcessing partition {idx + 1}/{len(partition_ids)}: {pid}\")\n",
    "        \n",
    "        # Extract data for this partition only\n",
    "        partition_df = df_with_pid.filter(F.col(\"_partition_id\") == pid)\n",
    "        \n",
    "        # Get partition column values (for adding back later)\n",
    "        partition_values = partition_df.select(partition_columns).first().asDict()\n",
    "        \n",
    "        # Compute shortest paths for this partition\n",
    "        shortest_paths = compute_shortest_paths_for_group(\n",
    "            partition_df.select(\"src\", \"dst\", \"cost\", \"next_node\"),\n",
    "            max_iterations=max_iterations_per_group,\n",
    "            checkpoint_interval=3\n",
    "        )\n",
    "        \n",
    "        # Add back partition columns\n",
    "        for col, val in partition_values.items():\n",
    "            shortest_paths = shortest_paths.withColumn(col, F.lit(val))\n",
    "        \n",
    "        # Materialize this partition's results\n",
    "        shortest_paths = shortest_paths.persist()\n",
    "        shortest_paths.count()  # Force materialization\n",
    "        \n",
    "        results.append(shortest_paths)\n",
    "    \n",
    "    # Union all results\n",
    "    if not results:\n",
    "        return df_shortcuts.limit(0)  # Empty DataFrame with same schema\n",
    "    \n",
    "    print(f\"\\nCombining results from {len(results)} partitions...\")\n",
    "    final_result = results[0]\n",
    "    for result_df in results[1:]:\n",
    "        final_result = final_result.unionByName(result_df)\n",
    "    \n",
    "    # Final materialization\n",
    "    final_result = final_result.persist()\n",
    "    final_result.count()\n",
    "    \n",
    "    # Clean up intermediate results\n",
    "    for result_df in results:\n",
    "        result_df.unpersist()\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "\n",
    "def merge_shortcuts_to_main_table(\n",
    "    main_df: DataFrame,\n",
    "    new_shortcuts: DataFrame,\n",
    "    partition_columns: list\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Merges newly computed shortcuts back into the main table.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Remove old shortcuts for the processed partitions\n",
    "    2. Add new shortcuts\n",
    "    \n",
    "    Args:\n",
    "        main_df: Main shortcut table\n",
    "        new_shortcuts: Newly computed shortcuts\n",
    "        partition_columns: Columns used for partitioning\n",
    "    \n",
    "    Returns:\n",
    "        Updated DataFrame with new shortcuts\n",
    "    \"\"\"\n",
    "    # Get the partition values that were recomputed\n",
    "    recomputed_partitions = new_shortcuts.select(partition_columns).distinct()\n",
    "    \n",
    "    # Remove old shortcuts for these partitions using left_anti join\n",
    "    remaining_df = main_df.alias(\"main\").join(\n",
    "        recomputed_partitions.alias(\"recomp\"),\n",
    "        [F.col(f\"main.{col}\") == F.col(f\"recomp.{col}\") for col in partition_columns],\n",
    "        \"left_anti\"\n",
    "    )\n",
    "    \n",
    "    # Combine with new shortcuts\n",
    "    updated_df = remaining_df.unionByName(new_shortcuts)\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "\n",
    "def example_pipeline(spark: SparkSession):\n",
    "    \"\"\"\n",
    "    Example usage in an iterative pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example: Create initial shortcuts table\n",
    "    data = [\n",
    "        (\"A\", \"B\", 1.0, \"B\", \"region1\"),\n",
    "        (\"B\", \"C\", 2.0, \"C\", \"region1\"),\n",
    "        (\"C\", \"D\", 1.0, \"D\", \"region1\"),\n",
    "        (\"A\", \"C\", 5.0, \"C\", \"region1\"),  # Suboptimal shortcut\n",
    "        (\"E\", \"F\", 1.0, \"F\", \"region2\"),\n",
    "        (\"F\", \"G\", 2.0, \"G\", \"region2\"),\n",
    "    ]\n",
    "    \n",
    "    shortcuts = spark.createDataFrame(\n",
    "        data, \n",
    "        [\"src\", \"dst\", \"cost\", \"next_node\", \"region_id\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Initial shortcuts:\")\n",
    "    shortcuts.show()\n",
    "    \n",
    "    # Iterative pipeline\n",
    "    for iteration in range(3):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ITERATION {iteration + 1}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Step 1: Filter (example: keep only cost < 10)\n",
    "        filtered = shortcuts.filter(F.col(\"cost\") < 10)\n",
    "        \n",
    "        # Step 2: Compute shortest paths per partition\n",
    "        new_shortcuts = compute_shortest_paths_per_partition(\n",
    "            filtered,\n",
    "            partition_columns=[\"region_id\"],\n",
    "            max_iterations_per_group=100\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nNew shortcuts computed:\")\n",
    "        new_shortcuts.show()\n",
    "        \n",
    "        # Step 3: Merge back to main table\n",
    "        shortcuts = merge_shortcuts_to_main_table(\n",
    "            shortcuts,\n",
    "            new_shortcuts,\n",
    "            partition_columns=[\"region_id\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nUpdated shortcuts table:\")\n",
    "        shortcuts.show()\n",
    "    \n",
    "    return shortcuts\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Increase driver memory to handle larger plans\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PureSparkShortestPath\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    spark.sparkContext.setCheckpointDir(\"checkpoints\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"PURE SPARK SQL SHORTEST PATH CALCULATOR\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    result = example_pipeline(spark)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULT\")\n",
    "    print(\"=\"*60)\n",
    "    result.show()\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ece8b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/16 16:59:17 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FIXED PURE SPARK SQL SHORTEST PATH CALCULATOR\n",
      "======================================================================\n",
      "Initial shortcuts:\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  A|  C| 5.0|        C|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PIPELINE ITERATION 1\n",
      "======================================================================\n",
      "\n",
      "Filtered: 6 shortcuts\n",
      "\n",
      "Processing 2 partitions...\n",
      "\n",
      "[Partition 1/2]\n",
      "  Partition has 4 edges\n",
      "  Starting with 4 paths\n",
      "  Iteration 1... 6 paths\n",
      "  Iteration 2... 6 paths\n",
      "  Converged! No new paths added.\n",
      "  Completed: 6 shortest paths computed\n",
      "\n",
      "[Partition 2/2]\n",
      "  Partition has 2 edges\n",
      "  Starting with 2 paths\n",
      "  Iteration 1... 3 paths\n",
      "  Iteration 2... 3 paths\n",
      "  Converged! No new paths added.\n",
      "  Completed: 3 shortest paths computed\n",
      "\n",
      "Combining 2 partition results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/16 16:59:21 ERROR Executor: Exception in task 0.0 in stage 1391.0 (TID 54360)\n",
      "org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///tmp/spark_partitions_5jxo1hf4/partition_0/part-00000-be0fbed6-869a-4dd9-bd93-016983a3c2ee-c000.snappy.parquet. File does not exist. It is possible the underlying files have been updated.\n",
      "You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:695)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/spark_partitions_5jxo1hf4/partition_0/part-00000-be0fbed6-869a-4dd9-bd93-016983a3c2ee-c000.snappy.parquet does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:214)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n",
      "\t... 21 more\n",
      "25/11/16 16:59:21 ERROR Executor: Exception in task 1.0 in stage 1391.0 (TID 54361)\n",
      "org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///tmp/spark_partitions_5jxo1hf4/partition_1/part-00000-bf0f56a9-89bb-4b7c-8ff6-bae1d2a1e37a-c000.snappy.parquet. File does not exist. It is possible the underlying files have been updated.\n",
      "You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:695)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/spark_partitions_5jxo1hf4/partition_1/part-00000-bf0f56a9-89bb-4b7c-8ff6-bae1d2a1e37a-c000.snappy.parquet does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:214)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n",
      "\t... 21 more\n",
      "25/11/16 16:59:21 WARN TaskSetManager: Lost task 0.0 in stage 1391.0 (TID 54360) (10.255.255.254 executor driver): org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///tmp/spark_partitions_5jxo1hf4/partition_0/part-00000-be0fbed6-869a-4dd9-bd93-016983a3c2ee-c000.snappy.parquet. File does not exist. It is possible the underlying files have been updated.\n",
      "You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:695)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/spark_partitions_5jxo1hf4/partition_0/part-00000-be0fbed6-869a-4dd9-bd93-016983a3c2ee-c000.snappy.parquet does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:214)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n",
      "\t... 21 more\n",
      "\n",
      "25/11/16 16:59:21 ERROR TaskSetManager: Task 0 in stage 1391.0 failed 1 times; aborting job\n",
      "25/11/16 16:59:21 WARN TaskSetManager: Lost task 1.0 in stage 1391.0 (TID 54361) (10.255.255.254 executor driver): org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///tmp/spark_partitions_5jxo1hf4/partition_1/part-00000-bf0f56a9-89bb-4b7c-8ff6-bae1d2a1e37a-c000.snappy.parquet. File does not exist. It is possible the underlying files have been updated.\n",
      "You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:695)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/spark_partitions_5jxo1hf4/partition_1/part-00000-bf0f56a9-89bb-4b7c-8ff6-bae1d2a1e37a-c000.snappy.parquet does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:214)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n",
      "\t... 21 more\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3331.count.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///tmp/spark_partitions_5jxo1hf4/partition_0/part-00000-be0fbed6-869a-4dd9-bd93-016983a3c2ee-c000.snappy.parquet. File does not exist. It is possible the underlying files have been updated.\nYou can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:695)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.FileNotFoundException: File file:/tmp/spark_partitions_5jxo1hf4/partition_0/part-00000-be0fbed6-869a-4dd9-bd93-016983a3c2ee-c000.snappy.parquet does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:38)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:214)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n\t... 21 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 294\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFIXED PURE SPARK SQL SHORTEST PATH CALCULATOR\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    292\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m result = \u001b[43mexample_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m    297\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFINAL RESULT\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 259\u001b[39m, in \u001b[36mexample_pipeline\u001b[39m\u001b[34m(spark)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# Compute shortest paths per partition\u001b[39;00m\n\u001b[32m    253\u001b[39m new_shortcuts = compute_shortest_paths_per_partition(\n\u001b[32m    254\u001b[39m     filtered,\n\u001b[32m    255\u001b[39m     partition_columns=[\u001b[33m\"\u001b[39m\u001b[33mregion_id\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    256\u001b[39m     max_iterations_per_group=\u001b[32m20\u001b[39m  \u001b[38;5;66;03m# Reduced for safety\u001b[39;00m\n\u001b[32m    257\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNew shortcuts computed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mnew_shortcuts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    260\u001b[39m new_shortcuts.show()\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m# Merge back\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:439\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o3331.count.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///tmp/spark_partitions_5jxo1hf4/partition_0/part-00000-be0fbed6-869a-4dd9-bd93-016983a3c2ee-c000.snappy.parquet. File does not exist. It is possible the underlying files have been updated.\nYou can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:695)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.FileNotFoundException: File file:/tmp/spark_partitions_5jxo1hf4/partition_0/part-00000-be0fbed6-869a-4dd9-bd93-016983a3c2ee-c000.snappy.parquet does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:38)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:214)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n\t... 21 more\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fixed Pure Spark SQL implementation for all-pairs shortest path calculation.\n",
    "Key fixes:\n",
    "1. Aggressive checkpointing every iteration to break lineage\n",
    "2. Write intermediate results to disk to force materialization\n",
    "3. No .cache() - use checkpoint instead\n",
    "4. Simplified convergence check to avoid subtract() operations\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "\n",
    "def compute_shortest_paths_for_group(\n",
    "    df_group: DataFrame, \n",
    "    max_iterations: int = None,\n",
    "    temp_dir: str = None\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes all-pairs shortest paths using pure Spark SQL with aggressive lineage breaking.\n",
    "    \n",
    "    The key insight: We must materialize to disk (not just cache) to truly break lineage.\n",
    "    \n",
    "    Args:\n",
    "        df_group: DataFrame with schema (src, dst, cost, next_node)\n",
    "        max_iterations: Maximum iterations (default: row count)\n",
    "        temp_dir: Temporary directory for checkpointing\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with shortest paths\n",
    "    \"\"\"\n",
    "    if max_iterations is None:\n",
    "        max_iterations = df_group.count()\n",
    "    \n",
    "    # Create temp directory if not provided\n",
    "    if temp_dir is None:\n",
    "        temp_dir = tempfile.mkdtemp(prefix=\"spark_shortest_path_\")\n",
    "    \n",
    "    # Initialize: rename columns\n",
    "    current_paths = df_group.select(\n",
    "        F.col(\"src\").alias(\"path_start\"),\n",
    "        F.col(\"dst\").alias(\"path_end\"),\n",
    "        F.col(\"cost\"),\n",
    "        F.col(\"next_node\")\n",
    "    )\n",
    "    \n",
    "    # Checkpoint immediately to break initial lineage\n",
    "    current_paths = current_paths.checkpoint(eager=True)\n",
    "    prev_count = current_paths.count()\n",
    "    \n",
    "    print(f\"  Starting with {prev_count} paths\")\n",
    "    \n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        print(f\"  Iteration {iteration}...\", end=\" \")\n",
    "        \n",
    "        # Step 1: Find new paths via self-join\n",
    "        new_paths = current_paths.alias(\"L\").join(\n",
    "            current_paths.alias(\"R\"),\n",
    "            F.col(\"L.path_end\") == F.col(\"R.path_start\"),\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            F.col(\"L.path_start\").alias(\"path_start\"),\n",
    "            F.col(\"R.path_end\").alias(\"path_end\"),\n",
    "            (F.col(\"L.cost\") + F.col(\"R.cost\")).alias(\"cost\"),\n",
    "            F.col(\"L.next_node\").alias(\"next_node\")\n",
    "        ).filter(F.col(\"path_start\") != F.col(\"path_end\"))\n",
    "        \n",
    "        # Step 2: Union with existing paths\n",
    "        all_paths = current_paths.unionByName(new_paths)\n",
    "        \n",
    "        # Step 3: Keep minimum cost per (start, end) pair\n",
    "        # Use window function to avoid additional join\n",
    "        from pyspark.sql.window import Window\n",
    "        window = Window.partitionBy(\"path_start\", \"path_end\").orderBy(\"cost\")\n",
    "        \n",
    "        next_paths = all_paths.withColumn(\"rank\", F.row_number().over(window)) \\\n",
    "                              .filter(F.col(\"rank\") == 1) \\\n",
    "                              .drop(\"rank\")\n",
    "        \n",
    "        # CRITICAL: Checkpoint EVERY iteration to break lineage\n",
    "        # Use eager=True to force immediate materialization\n",
    "        next_paths = next_paths.checkpoint(eager=True)\n",
    "        \n",
    "        # Count new paths\n",
    "        next_count = next_paths.count()\n",
    "        print(f\"{next_count} paths\")\n",
    "        \n",
    "        # Simple convergence check: if count didn't change, we're done\n",
    "        if next_count == prev_count:\n",
    "            print(f\"  Converged! No new paths added.\")\n",
    "            # Rename back to original schema\n",
    "            return next_paths.select(\n",
    "                F.col(\"path_start\").alias(\"src\"),\n",
    "                F.col(\"path_end\").alias(\"dst\"),\n",
    "                F.col(\"cost\"),\n",
    "                F.col(\"next_node\")\n",
    "            )\n",
    "        \n",
    "        prev_count = next_count\n",
    "        current_paths = next_paths\n",
    "    \n",
    "    print(f\"  Reached max iterations ({max_iterations})\")\n",
    "    \n",
    "    # Return with original column names\n",
    "    return current_paths.select(\n",
    "        F.col(\"path_start\").alias(\"src\"),\n",
    "        F.col(\"path_end\").alias(\"dst\"),\n",
    "        F.col(\"cost\"),\n",
    "        F.col(\"next_node\")\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_shortest_paths_per_partition(\n",
    "    df_shortcuts: DataFrame,\n",
    "    partition_columns: list,\n",
    "    max_iterations_per_group: int = None\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes shortest paths for each partition separately.\n",
    "    Processes one partition at a time to avoid memory issues.\n",
    "    \n",
    "    Args:\n",
    "        df_shortcuts: DataFrame with schema (src, dst, cost, next_node, partition_cols...)\n",
    "        partition_columns: Columns to partition by\n",
    "        max_iterations_per_group: Max iterations per group\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all shortest paths\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get unique partition values\n",
    "    partitions = df_shortcuts.select(partition_columns).distinct().collect()\n",
    "    \n",
    "    print(f\"\\nProcessing {len(partitions)} partitions...\")\n",
    "    \n",
    "    results = []\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"spark_partitions_\")\n",
    "    \n",
    "    try:\n",
    "        for idx, partition_row in enumerate(partitions):\n",
    "            print(f\"\\n[Partition {idx + 1}/{len(partitions)}]\")\n",
    "            \n",
    "            # Build filter condition for this partition\n",
    "            partition_dict = partition_row.asDict()\n",
    "            filter_condition = None\n",
    "            for col, val in partition_dict.items():\n",
    "                condition = (F.col(col) == F.lit(val))\n",
    "                filter_condition = condition if filter_condition is None else (filter_condition & condition)\n",
    "            \n",
    "            # Extract this partition's data\n",
    "            partition_df = df_shortcuts.filter(filter_condition)\n",
    "            partition_size = partition_df.count()\n",
    "            print(f\"  Partition has {partition_size} edges\")\n",
    "            \n",
    "            # Compute shortest paths for this partition\n",
    "            shortest_paths = compute_shortest_paths_for_group(\n",
    "                partition_df.select(\"src\", \"dst\", \"cost\", \"next_node\"),\n",
    "                max_iterations=max_iterations_per_group,\n",
    "                temp_dir=temp_dir\n",
    "            )\n",
    "            \n",
    "            # Add partition columns back\n",
    "            for col, val in partition_dict.items():\n",
    "                shortest_paths = shortest_paths.withColumn(col, F.lit(val))\n",
    "            \n",
    "            # Write this partition's result to temp location to break lineage\n",
    "            partition_result_path = f\"{temp_dir}/partition_{idx}\"\n",
    "            shortest_paths.write.mode(\"overwrite\").parquet(partition_result_path)\n",
    "            \n",
    "            # Read back (fully breaks lineage)\n",
    "            partition_result = df_shortcuts.sparkSession.read.parquet(partition_result_path)\n",
    "            results.append(partition_result)\n",
    "            \n",
    "            print(f\"  Completed: {partition_result.count()} shortest paths computed\")\n",
    "        \n",
    "        # Union all partition results\n",
    "        if not results:\n",
    "            return df_shortcuts.limit(0)\n",
    "        \n",
    "        print(f\"\\nCombining {len(results)} partition results...\")\n",
    "        final_result = results[0]\n",
    "        for result_df in results[1:]:\n",
    "            final_result = final_result.unionByName(result_df)\n",
    "        \n",
    "        return final_result\n",
    "        \n",
    "    finally:\n",
    "        # Cleanup temp directory\n",
    "        try:\n",
    "            shutil.rmtree(temp_dir)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "def merge_shortcuts_to_main_table(\n",
    "    main_df: DataFrame,\n",
    "    new_shortcuts: DataFrame,\n",
    "    partition_columns: list\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Merges new shortcuts back into main table.\n",
    "    Removes old shortcuts for recomputed partitions and adds new ones.\n",
    "    \"\"\"\n",
    "    recomputed_partitions = new_shortcuts.select(partition_columns).distinct()\n",
    "    \n",
    "    # Keep shortcuts from partitions that weren't recomputed\n",
    "    remaining_df = main_df.alias(\"main\").join(\n",
    "        recomputed_partitions.alias(\"recomp\"),\n",
    "        [F.col(f\"main.{col}\") == F.col(f\"recomp.{col}\") for col in partition_columns],\n",
    "        \"left_anti\"\n",
    "    )\n",
    "    \n",
    "    # Add new shortcuts\n",
    "    updated_df = remaining_df.unionByName(new_shortcuts)\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "\n",
    "def example_pipeline(spark: SparkSession):\n",
    "    \"\"\"Example usage demonstrating the fixed implementation.\"\"\"\n",
    "    \n",
    "    # Create test data\n",
    "    data = [\n",
    "        (\"A\", \"B\", 1.0, \"B\", \"region1\"),\n",
    "        (\"B\", \"C\", 2.0, \"C\", \"region1\"),\n",
    "        (\"C\", \"D\", 1.0, \"D\", \"region1\"),\n",
    "        (\"A\", \"C\", 5.0, \"C\", \"region1\"),  # Suboptimal path\n",
    "        (\"E\", \"F\", 1.0, \"F\", \"region2\"),\n",
    "        (\"F\", \"G\", 2.0, \"G\", \"region2\"),\n",
    "    ]\n",
    "    \n",
    "    shortcuts = spark.createDataFrame(\n",
    "        data, \n",
    "        [\"src\", \"dst\", \"cost\", \"next_node\", \"region_id\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Initial shortcuts:\")\n",
    "    shortcuts.show()\n",
    "    \n",
    "    # Iterative pipeline\n",
    "    for iteration in range(3):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"PIPELINE ITERATION {iteration + 1}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Filter\n",
    "        filtered = shortcuts.filter(F.col(\"cost\") < 10)\n",
    "        print(f\"\\nFiltered: {filtered.count()} shortcuts\")\n",
    "        \n",
    "        # Compute shortest paths per partition\n",
    "        new_shortcuts = compute_shortest_paths_per_partition(\n",
    "            filtered,\n",
    "            partition_columns=[\"region_id\"],\n",
    "            max_iterations_per_group=20  # Reduced for safety\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nNew shortcuts computed: {new_shortcuts.count()}\")\n",
    "        new_shortcuts.show()\n",
    "        \n",
    "        # Merge back\n",
    "        shortcuts = merge_shortcuts_to_main_table(\n",
    "            shortcuts,\n",
    "            new_shortcuts,\n",
    "            partition_columns=[\"region_id\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nUpdated table: {shortcuts.count()} total shortcuts\")\n",
    "    \n",
    "    return shortcuts\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration - still keep memory reasonable\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"FixedPureSparkShortestPath\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Set checkpoint directory\n",
    "    import os\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    spark.sparkContext.setCheckpointDir(checkpoint_dir)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"FIXED PURE SPARK SQL SHORTEST PATH CALCULATOR\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    result = example_pipeline(spark)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL RESULT\")\n",
    "    print(\"=\"*70)\n",
    "    result.show()\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a756bf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FIXED PURE SPARK SQL SHORTEST PATH CALCULATOR\n",
      "======================================================================\n",
      "Initial shortcuts:\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  A|  C| 5.0|        C|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PIPELINE ITERATION 1\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered: 6 shortcuts\n",
      "\n",
      "Processing 2 partitions...\n",
      "\n",
      "[Partition 1/2]\n",
      "  Partition has 4 edges\n",
      "  Starting with 4 paths\n",
      "  Iteration 1... 6 paths\n",
      "  Iteration 2... 6 paths\n",
      "  Converged! No new paths added.\n",
      "  Completed: 6 shortest paths computed\n",
      "\n",
      "[Partition 2/2]\n",
      "  Partition has 2 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Starting with 2 paths\n",
      "  Iteration 1... 3 paths\n",
      "  Iteration 2... 3 paths\n",
      "  Converged! No new paths added.\n",
      "  Completed: 3 shortest paths computed\n",
      "\n",
      "Combining 2 partition results...\n",
      "Cleaned up temp directory: /tmp/spark_partitions_3rnxn6u2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/16 17:01:10 ERROR Executor: Exception in task 1.0 in stage 1475.0 (TID 54706)\n",
      "org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///tmp/spark_partitions_3rnxn6u2/final_result/part-00001-79a4a7ad-091e-48cf-a971-bd814a29ac3e-c000.snappy.parquet. File does not exist. It is possible the underlying files have been updated.\n",
      "You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:695)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/spark_partitions_3rnxn6u2/final_result/part-00001-79a4a7ad-091e-48cf-a971-bd814a29ac3e-c000.snappy.parquet does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:214)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n",
      "\t... 21 more\n",
      "25/11/16 17:01:10 ERROR Executor: Exception in task 0.0 in stage 1475.0 (TID 54705)\n",
      "org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///tmp/spark_partitions_3rnxn6u2/final_result/part-00000-79a4a7ad-091e-48cf-a971-bd814a29ac3e-c000.snappy.parquet. File does not exist. It is possible the underlying files have been updated.\n",
      "You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:695)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/spark_partitions_3rnxn6u2/final_result/part-00000-79a4a7ad-091e-48cf-a971-bd814a29ac3e-c000.snappy.parquet does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:214)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n",
      "\t... 21 more\n",
      "25/11/16 17:01:10 WARN TaskSetManager: Lost task 0.0 in stage 1475.0 (TID 54705) (10.255.255.254 executor driver): org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///tmp/spark_partitions_3rnxn6u2/final_result/part-00000-79a4a7ad-091e-48cf-a971-bd814a29ac3e-c000.snappy.parquet. File does not exist. It is possible the underlying files have been updated.\n",
      "You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:695)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/spark_partitions_3rnxn6u2/final_result/part-00000-79a4a7ad-091e-48cf-a971-bd814a29ac3e-c000.snappy.parquet does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:214)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n",
      "\t... 21 more\n",
      "\n",
      "25/11/16 17:01:10 ERROR TaskSetManager: Task 0 in stage 1475.0 failed 1 times; aborting job\n",
      "25/11/16 17:01:10 WARN TaskSetManager: Lost task 1.0 in stage 1475.0 (TID 54706) (10.255.255.254 executor driver): org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///tmp/spark_partitions_3rnxn6u2/final_result/part-00001-79a4a7ad-091e-48cf-a971-bd814a29ac3e-c000.snappy.parquet. File does not exist. It is possible the underlying files have been updated.\n",
      "You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:695)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/spark_partitions_3rnxn6u2/final_result/part-00001-79a4a7ad-091e-48cf-a971-bd814a29ac3e-c000.snappy.parquet does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:214)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n",
      "\t... 21 more\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3726.count.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///tmp/spark_partitions_3rnxn6u2/final_result/part-00000-79a4a7ad-091e-48cf-a971-bd814a29ac3e-c000.snappy.parquet. File does not exist. It is possible the underlying files have been updated.\nYou can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:695)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.FileNotFoundException: File file:/tmp/spark_partitions_3rnxn6u2/final_result/part-00000-79a4a7ad-091e-48cf-a971-bd814a29ac3e-c000.snappy.parquet does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:38)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:214)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n\t... 21 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 299\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFIXED PURE SPARK SQL SHORTEST PATH CALCULATOR\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    297\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m result = \u001b[43mexample_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m    302\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFINAL RESULT\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 264\u001b[39m, in \u001b[36mexample_pipeline\u001b[39m\u001b[34m(spark)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# Compute shortest paths per partition\u001b[39;00m\n\u001b[32m    258\u001b[39m new_shortcuts = compute_shortest_paths_per_partition(\n\u001b[32m    259\u001b[39m     filtered,\n\u001b[32m    260\u001b[39m     partition_columns=[\u001b[33m\"\u001b[39m\u001b[33mregion_id\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    261\u001b[39m     max_iterations_per_group=\u001b[32m20\u001b[39m  \u001b[38;5;66;03m# Reduced for safety\u001b[39;00m\n\u001b[32m    262\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNew shortcuts computed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mnew_shortcuts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    265\u001b[39m new_shortcuts.show()\n\u001b[32m    267\u001b[39m \u001b[38;5;66;03m# Merge back\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:439\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/osm-routing-h3-pyspark/venv/lib/python3.12/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o3726.count.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///tmp/spark_partitions_3rnxn6u2/final_result/part-00000-79a4a7ad-091e-48cf-a971-bd814a29ac3e-c000.snappy.parquet. File does not exist. It is possible the underlying files have been updated.\nYou can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:695)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.FileNotFoundException: File file:/tmp/spark_partitions_3rnxn6u2/final_result/part-00000-79a4a7ad-091e-48cf-a971-bd814a29ac3e-c000.snappy.parquet does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:38)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:214)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n\t... 21 more\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fixed Pure Spark SQL implementation for all-pairs shortest path calculation.\n",
    "Key fixes:\n",
    "1. Aggressive checkpointing every iteration to break lineage\n",
    "2. Write intermediate results to disk to force materialization\n",
    "3. No .cache() - use checkpoint instead\n",
    "4. Simplified convergence check to avoid subtract() operations\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "\n",
    "def compute_shortest_paths_for_group(\n",
    "    df_group: DataFrame, \n",
    "    max_iterations: int = None,\n",
    "    temp_dir: str = None\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes all-pairs shortest paths using pure Spark SQL with aggressive lineage breaking.\n",
    "    \n",
    "    The key insight: We must materialize to disk (not just cache) to truly break lineage.\n",
    "    \n",
    "    Args:\n",
    "        df_group: DataFrame with schema (src, dst, cost, next_node)\n",
    "        max_iterations: Maximum iterations (default: row count)\n",
    "        temp_dir: Temporary directory for checkpointing\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with shortest paths\n",
    "    \"\"\"\n",
    "    if max_iterations is None:\n",
    "        max_iterations = df_group.count()\n",
    "    \n",
    "    # Create temp directory if not provided\n",
    "    if temp_dir is None:\n",
    "        temp_dir = tempfile.mkdtemp(prefix=\"spark_shortest_path_\")\n",
    "    \n",
    "    # Initialize: rename columns\n",
    "    current_paths = df_group.select(\n",
    "        F.col(\"src\").alias(\"path_start\"),\n",
    "        F.col(\"dst\").alias(\"path_end\"),\n",
    "        F.col(\"cost\"),\n",
    "        F.col(\"next_node\")\n",
    "    )\n",
    "    \n",
    "    # Checkpoint immediately to break initial lineage\n",
    "    current_paths = current_paths.checkpoint(eager=True)\n",
    "    prev_count = current_paths.count()\n",
    "    \n",
    "    print(f\"  Starting with {prev_count} paths\")\n",
    "    \n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        print(f\"  Iteration {iteration}...\", end=\" \")\n",
    "        \n",
    "        # Step 1: Find new paths via self-join\n",
    "        new_paths = current_paths.alias(\"L\").join(\n",
    "            current_paths.alias(\"R\"),\n",
    "            F.col(\"L.path_end\") == F.col(\"R.path_start\"),\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            F.col(\"L.path_start\").alias(\"path_start\"),\n",
    "            F.col(\"R.path_end\").alias(\"path_end\"),\n",
    "            (F.col(\"L.cost\") + F.col(\"R.cost\")).alias(\"cost\"),\n",
    "            F.col(\"L.next_node\").alias(\"next_node\")\n",
    "        ).filter(F.col(\"path_start\") != F.col(\"path_end\"))\n",
    "        \n",
    "        # Step 2: Union with existing paths\n",
    "        all_paths = current_paths.unionByName(new_paths)\n",
    "        \n",
    "        # Step 3: Keep minimum cost per (start, end) pair\n",
    "        # Use window function to avoid additional join\n",
    "        from pyspark.sql.window import Window\n",
    "        window = Window.partitionBy(\"path_start\", \"path_end\").orderBy(\"cost\")\n",
    "        \n",
    "        next_paths = all_paths.withColumn(\"rank\", F.row_number().over(window)) \\\n",
    "                              .filter(F.col(\"rank\") == 1) \\\n",
    "                              .drop(\"rank\")\n",
    "        \n",
    "        # CRITICAL: Checkpoint EVERY iteration to break lineage\n",
    "        # Use eager=True to force immediate materialization\n",
    "        next_paths = next_paths.checkpoint(eager=True)\n",
    "        \n",
    "        # Count new paths\n",
    "        next_count = next_paths.count()\n",
    "        print(f\"{next_count} paths\")\n",
    "        \n",
    "        # Simple convergence check: if count didn't change, we're done\n",
    "        if next_count == prev_count:\n",
    "            print(f\"  Converged! No new paths added.\")\n",
    "            # Rename back to original schema\n",
    "            return next_paths.select(\n",
    "                F.col(\"path_start\").alias(\"src\"),\n",
    "                F.col(\"path_end\").alias(\"dst\"),\n",
    "                F.col(\"cost\"),\n",
    "                F.col(\"next_node\")\n",
    "            )\n",
    "        \n",
    "        prev_count = next_count\n",
    "        current_paths = next_paths\n",
    "    \n",
    "    print(f\"  Reached max iterations ({max_iterations})\")\n",
    "    \n",
    "    # Return with original column names\n",
    "    return current_paths.select(\n",
    "        F.col(\"path_start\").alias(\"src\"),\n",
    "        F.col(\"path_end\").alias(\"dst\"),\n",
    "        F.col(\"cost\"),\n",
    "        F.col(\"next_node\")\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_shortest_paths_per_partition(\n",
    "    df_shortcuts: DataFrame,\n",
    "    partition_columns: list,\n",
    "    max_iterations_per_group: int = None\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes shortest paths for each partition separately.\n",
    "    Processes one partition at a time to avoid memory issues.\n",
    "    \n",
    "    Args:\n",
    "        df_shortcuts: DataFrame with schema (src, dst, cost, next_node, partition_cols...)\n",
    "        partition_columns: Columns to partition by\n",
    "        max_iterations_per_group: Max iterations per group\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all shortest paths\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get unique partition values\n",
    "    partitions = df_shortcuts.select(partition_columns).distinct().collect()\n",
    "    \n",
    "    print(f\"\\nProcessing {len(partitions)} partitions...\")\n",
    "    \n",
    "    results = []\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"spark_partitions_\")\n",
    "    \n",
    "    for idx, partition_row in enumerate(partitions):\n",
    "        print(f\"\\n[Partition {idx + 1}/{len(partitions)}]\")\n",
    "        \n",
    "        # Build filter condition for this partition\n",
    "        partition_dict = partition_row.asDict()\n",
    "        filter_condition = None\n",
    "        for col, val in partition_dict.items():\n",
    "            condition = (F.col(col) == F.lit(val))\n",
    "            filter_condition = condition if filter_condition is None else (filter_condition & condition)\n",
    "        \n",
    "        # Extract this partition's data\n",
    "        partition_df = df_shortcuts.filter(filter_condition)\n",
    "        partition_size = partition_df.count()\n",
    "        print(f\"  Partition has {partition_size} edges\")\n",
    "        \n",
    "        # Compute shortest paths for this partition\n",
    "        shortest_paths = compute_shortest_paths_for_group(\n",
    "            partition_df.select(\"src\", \"dst\", \"cost\", \"next_node\"),\n",
    "            max_iterations=max_iterations_per_group,\n",
    "            temp_dir=temp_dir\n",
    "        )\n",
    "        \n",
    "        # Add partition columns back\n",
    "        for col, val in partition_dict.items():\n",
    "            shortest_paths = shortest_paths.withColumn(col, F.lit(val))\n",
    "        \n",
    "        # Write this partition's result to temp location to break lineage\n",
    "        partition_result_path = f\"{temp_dir}/partition_{idx}\"\n",
    "        shortest_paths.write.mode(\"overwrite\").parquet(partition_result_path)\n",
    "        \n",
    "        # Read back (fully breaks lineage)\n",
    "        partition_result = df_shortcuts.sparkSession.read.parquet(partition_result_path)\n",
    "        results.append(partition_result)\n",
    "        \n",
    "        print(f\"  Completed: {partition_result.count()} shortest paths computed\")\n",
    "    \n",
    "    # Union all partition results\n",
    "    if not results:\n",
    "        return df_shortcuts.limit(0)\n",
    "    \n",
    "    print(f\"\\nCombining {len(results)} partition results...\")\n",
    "    final_result = results[0]\n",
    "    for result_df in results[1:]:\n",
    "        final_result = final_result.unionByName(result_df)\n",
    "    \n",
    "    # CRITICAL: Materialize the union before cleanup\n",
    "    # Write to a final location and read back\n",
    "    final_output_path = f\"{temp_dir}/final_result\"\n",
    "    final_result.write.mode(\"overwrite\").parquet(final_output_path)\n",
    "    final_result = df_shortcuts.sparkSession.read.parquet(final_output_path)\n",
    "    \n",
    "    # Now safe to cleanup temp directory\n",
    "    try:\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"Cleaned up temp directory: {temp_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not clean up temp directory {temp_dir}: {e}\")\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "\n",
    "def merge_shortcuts_to_main_table(\n",
    "    main_df: DataFrame,\n",
    "    new_shortcuts: DataFrame,\n",
    "    partition_columns: list\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Merges new shortcuts back into main table.\n",
    "    Removes old shortcuts for recomputed partitions and adds new ones.\n",
    "    \"\"\"\n",
    "    recomputed_partitions = new_shortcuts.select(partition_columns).distinct()\n",
    "    \n",
    "    # Keep shortcuts from partitions that weren't recomputed\n",
    "    remaining_df = main_df.alias(\"main\").join(\n",
    "        recomputed_partitions.alias(\"recomp\"),\n",
    "        [F.col(f\"main.{col}\") == F.col(f\"recomp.{col}\") for col in partition_columns],\n",
    "        \"left_anti\"\n",
    "    )\n",
    "    \n",
    "    # Add new shortcuts\n",
    "    updated_df = remaining_df.unionByName(new_shortcuts)\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "\n",
    "def example_pipeline(spark: SparkSession):\n",
    "    \"\"\"Example usage demonstrating the fixed implementation.\"\"\"\n",
    "    \n",
    "    # Create test data\n",
    "    data = [\n",
    "        (\"A\", \"B\", 1.0, \"B\", \"region1\"),\n",
    "        (\"B\", \"C\", 2.0, \"C\", \"region1\"),\n",
    "        (\"C\", \"D\", 1.0, \"D\", \"region1\"),\n",
    "        (\"A\", \"C\", 5.0, \"C\", \"region1\"),  # Suboptimal path\n",
    "        (\"E\", \"F\", 1.0, \"F\", \"region2\"),\n",
    "        (\"F\", \"G\", 2.0, \"G\", \"region2\"),\n",
    "    ]\n",
    "    \n",
    "    shortcuts = spark.createDataFrame(\n",
    "        data, \n",
    "        [\"src\", \"dst\", \"cost\", \"next_node\", \"region_id\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Initial shortcuts:\")\n",
    "    shortcuts.show()\n",
    "    \n",
    "    # Iterative pipeline\n",
    "    for iteration in range(3):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"PIPELINE ITERATION {iteration + 1}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Filter\n",
    "        filtered = shortcuts.filter(F.col(\"cost\") < 10)\n",
    "        print(f\"\\nFiltered: {filtered.count()} shortcuts\")\n",
    "        \n",
    "        # Compute shortest paths per partition\n",
    "        new_shortcuts = compute_shortest_paths_per_partition(\n",
    "            filtered,\n",
    "            partition_columns=[\"region_id\"],\n",
    "            max_iterations_per_group=20  # Reduced for safety\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nNew shortcuts computed: {new_shortcuts.count()}\")\n",
    "        new_shortcuts.show()\n",
    "        \n",
    "        # Merge back\n",
    "        shortcuts = merge_shortcuts_to_main_table(\n",
    "            shortcuts,\n",
    "            new_shortcuts,\n",
    "            partition_columns=[\"region_id\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nUpdated table: {shortcuts.count()} total shortcuts\")\n",
    "    \n",
    "    return shortcuts\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration - still keep memory reasonable\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"FixedPureSparkShortestPath\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Set checkpoint directory\n",
    "    import os\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    spark.sparkContext.setCheckpointDir(checkpoint_dir)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"FIXED PURE SPARK SQL SHORTEST PATH CALCULATOR\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    result = example_pipeline(spark)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL RESULT\")\n",
    "    print(\"=\"*70)\n",
    "    result.show()\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a7bfb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FIXED PURE SPARK SQL SHORTEST PATH CALCULATOR\n",
      "======================================================================\n",
      "Initial shortcuts:\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  A|  C| 5.0|        C|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PIPELINE ITERATION 1\n",
      "======================================================================\n",
      "\n",
      "Filtered: 6 shortcuts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2 partitions...\n",
      "\n",
      "[Partition 1/2]\n",
      "  Partition has 4 edges\n",
      "  Starting with 4 paths\n",
      "  Iteration 1... 6 paths\n",
      "  Iteration 2... 6 paths\n",
      "  Converged! No new paths added.\n",
      "  Completed: 6 shortest paths computed\n",
      "\n",
      "[Partition 2/2]\n",
      "  Partition has 2 edges\n",
      "  Starting with 2 paths\n",
      "  Iteration 1... 3 paths\n",
      "  Iteration 2... 3 paths\n",
      "  Converged! No new paths added.\n",
      "  Completed: 3 shortest paths computed\n",
      "\n",
      "Combining 2 partition results...\n",
      "\n",
      "New shortcuts computed: 9\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "Updated table: 9 total shortcuts\n",
      "\n",
      "======================================================================\n",
      "PIPELINE ITERATION 2\n",
      "======================================================================\n",
      "\n",
      "Filtered: 9 shortcuts\n",
      "\n",
      "Processing 2 partitions...\n",
      "\n",
      "[Partition 1/2]\n",
      "  Partition has 6 edges\n",
      "  Starting with 6 paths\n",
      "  Iteration 1... 6 paths\n",
      "  Converged! No new paths added.\n",
      "  Completed: 6 shortest paths computed\n",
      "\n",
      "[Partition 2/2]\n",
      "  Partition has 3 edges\n",
      "  Starting with 3 paths\n",
      "  Iteration 1... 3 paths\n",
      "  Converged! No new paths added.\n",
      "  Completed: 3 shortest paths computed\n",
      "\n",
      "Combining 2 partition results...\n",
      "\n",
      "New shortcuts computed: 9\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "Updated table: 9 total shortcuts\n",
      "\n",
      "======================================================================\n",
      "PIPELINE ITERATION 3\n",
      "======================================================================\n",
      "\n",
      "Filtered: 9 shortcuts\n",
      "\n",
      "Processing 2 partitions...\n",
      "\n",
      "[Partition 1/2]\n",
      "  Partition has 6 edges\n",
      "  Starting with 6 paths\n",
      "  Iteration 1... 6 paths\n",
      "  Converged! No new paths added.\n",
      "  Completed: 6 shortest paths computed\n",
      "\n",
      "[Partition 2/2]\n",
      "  Partition has 3 edges\n",
      "  Starting with 3 paths\n",
      "  Iteration 1... 3 paths\n",
      "  Converged! No new paths added.\n",
      "  Completed: 3 shortest paths computed\n",
      "\n",
      "Combining 2 partition results...\n",
      "\n",
      "New shortcuts computed: 9\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n",
      "\n",
      "Updated table: 9 total shortcuts\n",
      "\n",
      "======================================================================\n",
      "FINAL RESULT\n",
      "======================================================================\n",
      "+---+---+----+---------+---------+\n",
      "|src|dst|cost|next_node|region_id|\n",
      "+---+---+----+---------+---------+\n",
      "|  A|  B| 1.0|        B|  region1|\n",
      "|  A|  C| 3.0|        B|  region1|\n",
      "|  A|  D| 4.0|        B|  region1|\n",
      "|  B|  C| 2.0|        C|  region1|\n",
      "|  B|  D| 3.0|        C|  region1|\n",
      "|  C|  D| 1.0|        D|  region1|\n",
      "|  E|  F| 1.0|        F|  region2|\n",
      "|  E|  G| 3.0|        F|  region2|\n",
      "|  F|  G| 2.0|        G|  region2|\n",
      "+---+---+----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fixed Pure Spark SQL implementation for all-pairs shortest path calculation.\n",
    "Key fixes:\n",
    "1. Aggressive checkpointing with eager=True to break lineage\n",
    "2. Use Spark's checkpoint mechanism instead of manual file operations\n",
    "3. Simplified convergence check\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def compute_shortest_paths_for_group(\n",
    "    df_group: DataFrame, \n",
    "    max_iterations: int = None\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes all-pairs shortest paths using pure Spark SQL with aggressive lineage breaking.\n",
    "    \n",
    "    Uses eager checkpointing every iteration to prevent execution plan explosion.\n",
    "    \n",
    "    Args:\n",
    "        df_group: DataFrame with schema (src, dst, cost, next_node)\n",
    "        max_iterations: Maximum iterations (default: row count)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with shortest paths\n",
    "    \"\"\"\n",
    "    if max_iterations is None:\n",
    "        max_iterations = df_group.count()\n",
    "    \n",
    "    # Initialize: rename columns\n",
    "    current_paths = df_group.select(\n",
    "        F.col(\"src\").alias(\"path_start\"),\n",
    "        F.col(\"dst\").alias(\"path_end\"),\n",
    "        F.col(\"cost\"),\n",
    "        F.col(\"next_node\")\n",
    "    )\n",
    "    \n",
    "    # Checkpoint immediately to break initial lineage\n",
    "    current_paths = current_paths.checkpoint(eager=True)\n",
    "    prev_count = current_paths.count()\n",
    "    \n",
    "    print(f\"  Starting with {prev_count} paths\")\n",
    "    \n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        print(f\"  Iteration {iteration}...\", end=\" \")\n",
    "        \n",
    "        # Step 1: Find new paths via self-join\n",
    "        new_paths = current_paths.alias(\"L\").join(\n",
    "            current_paths.alias(\"R\"),\n",
    "            F.col(\"L.path_end\") == F.col(\"R.path_start\"),\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            F.col(\"L.path_start\").alias(\"path_start\"),\n",
    "            F.col(\"R.path_end\").alias(\"path_end\"),\n",
    "            (F.col(\"L.cost\") + F.col(\"R.cost\")).alias(\"cost\"),\n",
    "            F.col(\"L.next_node\").alias(\"next_node\")\n",
    "        ).filter(F.col(\"path_start\") != F.col(\"path_end\"))\n",
    "        \n",
    "        # Step 2: Union with existing paths\n",
    "        all_paths = current_paths.unionByName(new_paths)\n",
    "        \n",
    "        # Step 3: Keep minimum cost per (start, end) pair\n",
    "        # Use window function to avoid additional join\n",
    "        from pyspark.sql.window import Window\n",
    "        window = Window.partitionBy(\"path_start\", \"path_end\").orderBy(\"cost\")\n",
    "        \n",
    "        next_paths = all_paths.withColumn(\"rank\", F.row_number().over(window)) \\\n",
    "                              .filter(F.col(\"rank\") == 1) \\\n",
    "                              .drop(\"rank\")\n",
    "        \n",
    "        # CRITICAL: Checkpoint EVERY iteration to break lineage\n",
    "        # Use eager=True to force immediate materialization\n",
    "        next_paths = next_paths.checkpoint(eager=True)\n",
    "        \n",
    "        # Count new paths\n",
    "        next_count = next_paths.count()\n",
    "        print(f\"{next_count} paths\")\n",
    "        \n",
    "        # Simple convergence check: if count didn't change, we're done\n",
    "        if next_count == prev_count:\n",
    "            print(f\"  Converged! No new paths added.\")\n",
    "            # Rename back to original schema\n",
    "            return next_paths.select(\n",
    "                F.col(\"path_start\").alias(\"src\"),\n",
    "                F.col(\"path_end\").alias(\"dst\"),\n",
    "                F.col(\"cost\"),\n",
    "                F.col(\"next_node\")\n",
    "            )\n",
    "        \n",
    "        prev_count = next_count\n",
    "        current_paths = next_paths\n",
    "    \n",
    "    print(f\"  Reached max iterations ({max_iterations})\")\n",
    "    \n",
    "    # Return with original column names\n",
    "    return current_paths.select(\n",
    "        F.col(\"path_start\").alias(\"src\"),\n",
    "        F.col(\"path_end\").alias(\"dst\"),\n",
    "        F.col(\"cost\"),\n",
    "        F.col(\"next_node\")\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_shortest_paths_per_partition(\n",
    "    df_shortcuts: DataFrame,\n",
    "    partition_columns: list,\n",
    "    max_iterations_per_group: int = None,\n",
    "    intermediate_path: str = None\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes shortest paths for each partition separately.\n",
    "    Processes one partition at a time to avoid memory issues.\n",
    "    \n",
    "    Args:\n",
    "        df_shortcuts: DataFrame with schema (src, dst, cost, next_node, partition_cols...)\n",
    "        partition_columns: Columns to partition by\n",
    "        max_iterations_per_group: Max iterations per group\n",
    "        intermediate_path: Path to store intermediate results (will be cleaned up after)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all shortest paths\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get unique partition values\n",
    "    partitions = df_shortcuts.select(partition_columns).distinct().collect()\n",
    "    \n",
    "    print(f\"\\nProcessing {len(partitions)} partitions...\")\n",
    "    \n",
    "    # Use provided path or create one\n",
    "    if intermediate_path is None:\n",
    "        intermediate_path = f\"temp_shortest_paths_{id(df_shortcuts)}\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, partition_row in enumerate(partitions):\n",
    "        print(f\"\\n[Partition {idx + 1}/{len(partitions)}]\")\n",
    "        \n",
    "        # Build filter condition for this partition\n",
    "        partition_dict = partition_row.asDict()\n",
    "        filter_condition = None\n",
    "        for col, val in partition_dict.items():\n",
    "            condition = (F.col(col) == F.lit(val))\n",
    "            filter_condition = condition if filter_condition is None else (filter_condition & condition)\n",
    "        \n",
    "        # Extract this partition's data\n",
    "        partition_df = df_shortcuts.filter(filter_condition)\n",
    "        partition_size = partition_df.count()\n",
    "        print(f\"  Partition has {partition_size} edges\")\n",
    "        \n",
    "        # Compute shortest paths for this partition\n",
    "        shortest_paths = compute_shortest_paths_for_group(\n",
    "            partition_df.select(\"src\", \"dst\", \"cost\", \"next_node\"),\n",
    "            max_iterations=max_iterations_per_group\n",
    "        )\n",
    "        \n",
    "        # Add partition columns back\n",
    "        for col, val in partition_dict.items():\n",
    "            shortest_paths = shortest_paths.withColumn(col, F.lit(val))\n",
    "        \n",
    "        # Checkpoint this partition's result to break lineage completely\n",
    "        shortest_paths = shortest_paths.checkpoint(eager=True)\n",
    "        \n",
    "        results.append(shortest_paths)\n",
    "        \n",
    "        print(f\"  Completed: {shortest_paths.count()} shortest paths computed\")\n",
    "    \n",
    "    # Union all partition results\n",
    "    if not results:\n",
    "        return df_shortcuts.limit(0)\n",
    "    \n",
    "    print(f\"\\nCombining {len(results)} partition results...\")\n",
    "    final_result = results[0]\n",
    "    for result_df in results[1:]:\n",
    "        final_result = final_result.unionByName(result_df)\n",
    "    \n",
    "    # Checkpoint the final union to break lineage\n",
    "    final_result = final_result.checkpoint(eager=True)\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "\n",
    "def merge_shortcuts_to_main_table(\n",
    "    main_df: DataFrame,\n",
    "    new_shortcuts: DataFrame,\n",
    "    partition_columns: list\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Merges new shortcuts back into main table.\n",
    "    Removes old shortcuts for recomputed partitions and adds new ones.\n",
    "    \"\"\"\n",
    "    recomputed_partitions = new_shortcuts.select(partition_columns).distinct()\n",
    "    \n",
    "    # Keep shortcuts from partitions that weren't recomputed\n",
    "    remaining_df = main_df.alias(\"main\").join(\n",
    "        recomputed_partitions.alias(\"recomp\"),\n",
    "        [F.col(f\"main.{col}\") == F.col(f\"recomp.{col}\") for col in partition_columns],\n",
    "        \"left_anti\"\n",
    "    )\n",
    "    \n",
    "    # Add new shortcuts\n",
    "    updated_df = remaining_df.unionByName(new_shortcuts)\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "\n",
    "def example_pipeline(spark: SparkSession):\n",
    "    \"\"\"Example usage demonstrating the fixed implementation.\"\"\"\n",
    "    \n",
    "    # Create test data\n",
    "    data = [\n",
    "        (\"A\", \"B\", 1.0, \"B\", \"region1\"),\n",
    "        (\"B\", \"C\", 2.0, \"C\", \"region1\"),\n",
    "        (\"C\", \"D\", 1.0, \"D\", \"region1\"),\n",
    "        (\"A\", \"C\", 5.0, \"C\", \"region1\"),  # Suboptimal path\n",
    "        (\"E\", \"F\", 1.0, \"F\", \"region2\"),\n",
    "        (\"F\", \"G\", 2.0, \"G\", \"region2\"),\n",
    "    ]\n",
    "    \n",
    "    shortcuts = spark.createDataFrame(\n",
    "        data, \n",
    "        [\"src\", \"dst\", \"cost\", \"next_node\", \"region_id\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Initial shortcuts:\")\n",
    "    shortcuts.show()\n",
    "    \n",
    "    # Iterative pipeline\n",
    "    for iteration in range(3):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"PIPELINE ITERATION {iteration + 1}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Filter\n",
    "        filtered = shortcuts.filter(F.col(\"cost\") < 10)\n",
    "        print(f\"\\nFiltered: {filtered.count()} shortcuts\")\n",
    "        \n",
    "        # Compute shortest paths per partition\n",
    "        new_shortcuts = compute_shortest_paths_per_partition(\n",
    "            filtered,\n",
    "            partition_columns=[\"region_id\"],\n",
    "            max_iterations_per_group=20  # Reduced for safety\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nNew shortcuts computed: {new_shortcuts.count()}\")\n",
    "        new_shortcuts.show()\n",
    "        \n",
    "        # Merge back\n",
    "        shortcuts = merge_shortcuts_to_main_table(\n",
    "            shortcuts,\n",
    "            new_shortcuts,\n",
    "            partition_columns=[\"region_id\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nUpdated table: {shortcuts.count()} total shortcuts\")\n",
    "    \n",
    "    return shortcuts\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration - still keep memory reasonable\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"FixedPureSparkShortestPath\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Set checkpoint directory\n",
    "    import os\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    spark.sparkContext.setCheckpointDir(checkpoint_dir)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"FIXED PURE SPARK SQL SHORTEST PATH CALCULATOR\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    result = example_pipeline(spark)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL RESULT\")\n",
    "    print(\"=\"*70)\n",
    "    result.show()\n",
    "    \n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
