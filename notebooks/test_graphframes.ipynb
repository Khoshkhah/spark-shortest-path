{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphFrames Test Notebook\n",
    "\n",
    "This notebook tests the installation and basic functionality of GraphFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/20 19:07:26 WARN Utils: Your hostname, Bamdad-Beast, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/11/20 19:07:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "https://repos.spark-packages.org added as a remote repository with the name: repo-1\n",
      ":: loading settings :: url = jar:file:/home/kaveh/projects/spark-shortest-path/venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/kaveh/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/kaveh/.ivy2.5.2/jars\n",
      "io.graphframes#graphframes-spark4_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-136faccc-dc93-451c-aa5f-fc7ec255b760;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.graphframes#graphframes-spark4_2.13;0.10.0 in central\n",
      "\tfound io.graphframes#graphframes-graphx-spark4_2.13;0.10.0 in central\n",
      ":: resolution report :: resolve 83ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tio.graphframes#graphframes-graphx-spark4_2.13;0.10.0 from central in [default]\n",
      "\tio.graphframes#graphframes-spark4_2.13;0.10.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-136faccc-dc93-451c-aa5f-fc7ec255b760\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "25/11/20 19:07:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.0.1\n",
      "Scala Version: version 2.13.16\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from graphframes import GraphFrame\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Configure Spark with the Spark 4.0 GraphFrames JAR\n",
    "#GRAPH_FRAMES_PACKAGE = \"io.graphframes:graphframes-spark4_2.13:0.10.0\"\n",
    "\n",
    "#spark = SparkSession.builder \\\n",
    "#    .appName(\"GraphFrames Test\") \\\n",
    "#    .config(\"spark.jars.packages\", GRAPH_FRAMES_PACKAGE) \\\n",
    "#    .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GraphFrames Test\") \\\n",
    "    .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.graphframes:graphframes-spark4_2.13:0.10.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Scala Version: {spark.sparkContext._gateway.jvm.scala.util.Properties.versionString()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaveh/projects/spark-shortest-path/venv/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py:146: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "25/11/20 19:13:53 WARN ShortestPaths: Returned DataFrame is persistent and materialized!\n",
      "/home/kaveh/projects/spark-shortest-path/venv/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py:128: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "ename": "PySparkAttributeError",
     "evalue": "[ATTRIBUTE_NOT_SUPPORTED] Attribute `run` is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkAttributeError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m g \u001b[38;5;241m=\u001b[39m GraphFrame(v, e)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 5. Run Shortest Paths\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshortestPaths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlandmarks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m()\n\u001b[1;32m     28\u001b[0m results\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistances\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/projects/spark-shortest-path/venv/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py:971\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m--> 971\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkAttributeError(\n\u001b[1;32m    972\u001b[0m             errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mATTRIBUTE_NOT_SUPPORTED\u001b[39m\u001b[38;5;124m\"\u001b[39m, messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattr_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: name}\n\u001b[1;32m    973\u001b[0m         )\n\u001b[1;32m    974\u001b[0m     jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mPySparkAttributeError\u001b[0m: [ATTRIBUTE_NOT_SUPPORTED] Attribute `run` is not supported."
     ]
    }
   ],
   "source": [
    "# 2. Create the Vertices (Users)\n",
    "v = spark.createDataFrame([\n",
    "    (\"a\", \"Alice\", 34),\n",
    "    (\"b\", \"Bob\", 36),\n",
    "    (\"c\", \"Charlie\", 30),\n",
    "    (\"d\", \"David\", 29),\n",
    "    (\"e\", \"Esther\", 32),\n",
    "    (\"f\", \"Fanny\", 36)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# 3. Create the Edges (Relationships)\n",
    "e = spark.createDataFrame([\n",
    "    (\"a\", \"b\", \"friend\"),\n",
    "    (\"b\", \"c\", \"follow\"),\n",
    "    (\"c\", \"b\", \"follow\"),\n",
    "    (\"f\", \"c\", \"follow\"),\n",
    "    (\"e\", \"f\", \"follow\"),\n",
    "    (\"e\", \"d\", \"friend\"),\n",
    "    (\"d\", \"a\", \"friend\"),\n",
    "    (\"a\", \"e\", \"friend\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "# 4. Create the GraphFrame\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# 5. Run Shortest Paths\n",
    "results = g.shortestPaths(landmarks=[\"a\", \"d\"]).run()\n",
    "results.select(\"id\", \"distances\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple graph\n",
    "# Vertices DataFrame\n",
    "v = spark.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 30),\n",
    "  (\"d\", \"David\", 29),\n",
    "  (\"e\", \"Esther\", 32),\n",
    "  (\"f\", \"Fanny\", 36),\n",
    "  (\"g\", \"Gabby\", 60)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Edges DataFrame\n",
    "e = spark.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "  (\"f\", \"c\", \"follow\"),\n",
    "  (\"e\", \"f\", \"follow\"),\n",
    "  (\"e\", \"d\", \"friend\"),\n",
    "  (\"d\", \"a\", \"friend\"),\n",
    "  (\"a\", \"e\", \"friend\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "# Create a GraphFrame\n",
    "g = GraphFrame(v, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertices:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  a|  Alice| 34|\n",
      "|  b|    Bob| 36|\n",
      "|  c|Charlie| 30|\n",
      "|  d|  David| 29|\n",
      "|  e| Esther| 32|\n",
      "|  f|  Fanny| 36|\n",
      "|  g|  Gabby| 60|\n",
      "+---+-------+---+\n",
      "\n",
      "Edges:\n",
      "+---+---+------------+\n",
      "|src|dst|relationship|\n",
      "+---+---+------------+\n",
      "|  a|  b|      friend|\n",
      "|  b|  c|      follow|\n",
      "|  c|  b|      follow|\n",
      "|  f|  c|      follow|\n",
      "|  e|  f|      follow|\n",
      "|  e|  d|      friend|\n",
      "|  d|  a|      friend|\n",
      "|  a|  e|      friend|\n",
      "+---+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display vertices and edges\n",
    "print(\"Vertices:\")\n",
    "g.vertices.show()\n",
    "\n",
    "print(\"Edges:\")\n",
    "g.edges.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PageRank...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 19:07:37 WARN ShippableVertexPartitionOps: Joining two VertexPartitions with different indexes is slow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|           pagerank|\n",
      "+---+-------------------+\n",
      "|  c| 2.7339209603658534|\n",
      "|  b|  2.514646227134146|\n",
      "|  a|0.48915269944105694|\n",
      "|  e|0.40545134654471543|\n",
      "|  d|0.34304852959857723|\n",
      "|  f|0.34304852959857723|\n",
      "|  g|0.17073170731707318|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 19:07:39 WARN PageRank: Returned DataFrame is persistent and materialized!\n"
     ]
    }
   ],
   "source": [
    "# Run PageRank\n",
    "print(\"Running PageRank...\")\n",
    "results = g.pageRank(resetProbability=0.15, maxIter=5)\n",
    "results.vertices.select(\"id\", \"pagerank\").orderBy(F.desc(\"pagerank\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Connected Components...\n",
      "+---+------------+\n",
      "| id|   component|\n",
      "+---+------------+\n",
      "|  g|146028888064|\n",
      "|  c|412316860416|\n",
      "|  a|412316860416|\n",
      "|  f|412316860416|\n",
      "|  d|412316860416|\n",
      "|  e|412316860416|\n",
      "|  b|412316860416|\n",
      "+---+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 19:07:45 WARN ConnectedComponents$: Returned DataFrame is persistent and materialized!\n"
     ]
    }
   ],
   "source": [
    "# Run Connected Components\n",
    "print(\"Running Connected Components...\")\n",
    "spark.sparkContext.setCheckpointDir(\"../src/checkpoints\")\n",
    "cc = g.connectedComponents()\n",
    "cc.select(\"id\", \"component\").orderBy(\"component\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motif Finding (a -> b -> c)...\n",
      "+----------------+--------------+----------------+--------------+----------------+\n",
      "|               a|             e|               b|            e2|               c|\n",
      "+----------------+--------------+----------------+--------------+----------------+\n",
      "| {e, Esther, 32}|{e, d, friend}|  {d, David, 29}|{d, a, friend}|  {a, Alice, 34}|\n",
      "|  {d, David, 29}|{d, a, friend}|  {a, Alice, 34}|{a, b, friend}|    {b, Bob, 36}|\n",
      "|  {f, Fanny, 36}|{f, c, follow}|{c, Charlie, 30}|{c, b, follow}|    {b, Bob, 36}|\n",
      "|    {b, Bob, 36}|{b, c, follow}|{c, Charlie, 30}|{c, b, follow}|    {b, Bob, 36}|\n",
      "|{c, Charlie, 30}|{c, b, follow}|    {b, Bob, 36}|{b, c, follow}|{c, Charlie, 30}|\n",
      "|  {a, Alice, 34}|{a, b, friend}|    {b, Bob, 36}|{b, c, follow}|{c, Charlie, 30}|\n",
      "| {e, Esther, 32}|{e, f, follow}|  {f, Fanny, 36}|{f, c, follow}|{c, Charlie, 30}|\n",
      "|  {a, Alice, 34}|{a, e, friend}| {e, Esther, 32}|{e, d, friend}|  {d, David, 29}|\n",
      "|  {d, David, 29}|{d, a, friend}|  {a, Alice, 34}|{a, e, friend}| {e, Esther, 32}|\n",
      "|  {a, Alice, 34}|{a, e, friend}| {e, Esther, 32}|{e, f, follow}|  {f, Fanny, 36}|\n",
      "+----------------+--------------+----------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Motif Finding\n",
    "print(\"Motif Finding (a -> b -> c)...\")\n",
    "motifs = g.find(\"(a)-[e]->(b); (b)-[e2]->(c)\")\n",
    "motifs.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
