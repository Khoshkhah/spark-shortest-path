{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd1fce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-21-openjdk-amd64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fead58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "import h3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab1e65",
   "metadata": {},
   "source": [
    "## 1- Initialize spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7af38781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_spark(app_name: str = \"AllPairsShortestPath\") -> SparkSession:\n",
    "    \"\"\"\n",
    "    Initializes and returns a SparkSession.\n",
    "    \"\"\"\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(app_name)\n",
    "        .config(\"spark.driver.memory\", \"8g\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    spark.sparkContext.setCheckpointDir(\"checkpoints\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49abb778",
   "metadata": {},
   "source": [
    "## 2- Read edges data and initialize shortcuts table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf6bbba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_edges(spark: SparkSession, file_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Reads edges from a CSV file into a DataFrame.\n",
    "    \"\"\"\n",
    "    edges_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    edges_df = edges_df.select(\"id\", \"incoming_cell\", \"outgoing_cell\", \"lca_res\")\n",
    "    return edges_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33de3f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_shortcuts_table(spark: SparkSession, file_path: str, edges_cost_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Creates the initial shortcuts table from the edges DataFrame.\n",
    "    \"\"\"\n",
    "    shortcuts_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    shortcuts_df = shortcuts_df.select(\"incoming_edge\", \"outgoing_edge\")\n",
    "    # Add new via_edge column\n",
    "    shortcuts_df = shortcuts_df.withColumn(\"via_edge\", F.col(\"outgoing_edge\"))\n",
    "    \n",
    "    # shortcuts_df.show(5)\n",
    "    shortcuts_df = shortcuts_df.join(\n",
    "        edges_cost_df.select(\"id\", \"cost\"), \n",
    "        shortcuts_df.incoming_edge == edges_cost_df.id, \n",
    "        \"left\"\n",
    "    ).drop(edges_cost_df.id)\n",
    "   \n",
    "    return shortcuts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c691c3",
   "metadata": {},
   "source": [
    "## 3- Update edges cost by dummy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af10a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=DoubleType())\n",
    "def dummy_cost(length, maxspeed):\n",
    "    \"\"\"Calculates cost based on length and maxspeed.\"\"\"\n",
    "    if maxspeed <= 0:\n",
    "        return float('inf')  # Return infinity for invalid speeds\n",
    "    # Example calculation: Cost is proportional to length and inversely proportional to speed\n",
    "    return float(length) / float(maxspeed)\n",
    "\n",
    "def update_dummy_costs_for_edges(spark: SparkSession, file_path: str, edges_df: DataFrame):\n",
    "    \"\"\"\n",
    "    Adds a dummy cost column to the edges DataFrame.\n",
    "    \"\"\"\n",
    "    edges_df_cost = spark.read.csv(file_path, header=True, inferSchema=True).select(\"id\", \"length\", \"maxspeed\")\n",
    "    edges_df_cost = edges_df_cost.withColumn(\n",
    "        \"cost\", \n",
    "        dummy_cost(F.col(\"length\"), F.col(\"maxspeed\"))\n",
    "    )\n",
    "    edges_df = edges_df.drop(\"cost\")  # Remove existing cost column if present\n",
    "    edges_df = edges_df.join(edges_df_cost.select(\"id\", \"cost\"), on=\"id\", how=\"left\")\n",
    "    return edges_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1700d1",
   "metadata": {},
   "source": [
    "## 4- Add info \"via_cell\", \"via_res\" and \"lca_res\" to shortcuts table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "419b68d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCA - Lowest Common Ancestor resolution\n",
    "@F.udf(StringType())\n",
    "def find_LCA(cell1, cell2):\n",
    "    cell1_res = h3.get_resolution(cell1)\n",
    "    cell2_res = h3.get_resolution(cell2)\n",
    "    lca_res = min(cell1_res, cell2_res)\n",
    "    while h3.cell_to_parent(cell1, lca_res) != h3.cell_to_parent(cell2, lca_res) and lca_res > 0:\n",
    "        lca_res -= 1\n",
    "    mycell = h3.cell_to_parent(cell1, lca_res)\n",
    "    if mycell == h3.cell_to_parent(cell2, lca_res):\n",
    "        return mycell\n",
    "    else:\n",
    "        return None  # maybe return 0 is better option\n",
    "\n",
    "# Resolution of a cell\n",
    "@F.udf(IntegerType())  \n",
    "def find_resolution(cell):\n",
    "    if cell is None:\n",
    "        return -1\n",
    "    return h3.get_resolution(cell)\n",
    "\n",
    "def add_info_for_shortcuts(spark: SparkSession, shortcuts_df: DataFrame, edges_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adds additional information to the shortcuts DataFrame.\n",
    "    Add incoming_cell and lca_res from incoming_edge to shortcuts_df\n",
    "    Add outgoing_cell and lca_res from outgoing_edge to shortcuts_df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop existing columns if present: lca_res, via_cell, via_res\n",
    "    for col in [\"lca_res\", \"via_cell\", \"via_res\"]:\n",
    "        if col in shortcuts_df.columns:\n",
    "            shortcuts_df = shortcuts_df.drop(col)\n",
    "            \n",
    "    # Join to get incoming_cell and lca for incoming_edge\n",
    "    shortcuts_df = shortcuts_df.join(\n",
    "        edges_df.select(\n",
    "            F.col(\"id\").alias(\"incoming_edge_id\"),\n",
    "            F.col(\"incoming_cell\").alias(\"incoming_cell_in\"),\n",
    "            F.col(\"lca_res\").alias(\"lca_res_in\")\n",
    "        ),\n",
    "        shortcuts_df.incoming_edge == F.col(\"incoming_edge_id\"),\n",
    "        \"left\"\n",
    "    ).drop(\"incoming_edge_id\")\n",
    "    \n",
    "    # Join to get outgoing_cell and lca for outgoing_edge\n",
    "    shortcuts_df = shortcuts_df.join(\n",
    "        edges_df.select(\n",
    "            F.col(\"id\").alias(\"outgoing_edge_id\"),\n",
    "            F.col(\"outgoing_cell\").alias(\"outgoing_cell_out\"),\n",
    "            F.col(\"lca_res\").alias(\"lca_res_out\")\n",
    "        ),\n",
    "        shortcuts_df.outgoing_edge == F.col(\"outgoing_edge_id\"),\n",
    "        \"left\"\n",
    "    ).drop(\"outgoing_edge_id\")\n",
    "    \n",
    "    # Add lca_res column as the maximum of lca_res_in and lca_res_out\n",
    "    shortcuts_df = shortcuts_df.withColumn(\n",
    "        \"lca_res\",\n",
    "        F.greatest(F.col(\"lca_res_in\"), F.col(\"lca_res_out\"))\n",
    "    )\n",
    "    # Drop intermediate lca_res_in and lca_res_out columns\n",
    "    shortcuts_df = shortcuts_df.drop(\"lca_res_in\", \"lca_res_out\")\n",
    "    \n",
    "    # Add via_cell column as LCA of incoming_cell_in and outgoing_cell_out\n",
    "    shortcuts_df = shortcuts_df.withColumn(\n",
    "        \"via_cell\", find_LCA(F.col(\"incoming_cell_in\"), F.col(\"outgoing_cell_out\"))\n",
    "    )   \n",
    "    shortcuts_df = shortcuts_df.withColumn(\n",
    "        \"via_res\", find_resolution(F.col(\"via_cell\"))\n",
    "    )\n",
    "    # Drop intermediate incoming_cell_in and outgoing_cell_out columns\n",
    "    shortcuts_df = shortcuts_df.drop(\"incoming_cell_in\", \"outgoing_cell_out\")   \n",
    "    \n",
    "    return shortcuts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7abc15f",
   "metadata": {},
   "source": [
    "## 5- Filter shortcuts table based on current resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d02c11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_shortcuts_by_resolution(shortcuts_df: DataFrame, current_res: int) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the shortcuts DataFrame based on the current resolution.\n",
    "    Keeps only the shortcuts where lca_res is greater than or equal to current_res.\n",
    "    \"\"\"\n",
    "    filtered_shortcuts_df = shortcuts_df.filter(\n",
    "        (F.col(\"lca_res\") <= current_res) & (F.col(\"via_res\") >= current_res)\n",
    "    )\n",
    "    return filtered_shortcuts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f7da0",
   "metadata": {},
   "source": [
    "## 6- add cell container for each shortcut based on current resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8a6c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_parent_cell_at_resolution(shortcuts_df: DataFrame, current_resolution: int) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a cell container column to shortcuts based on the current resolution.\n",
    "    \n",
    "    The cell container is computed as the parent cell of the via_cell at the \n",
    "    current resolution level. This allows grouping shortcuts by their spatial\n",
    "    containment at different H3 resolution levels.\n",
    "    \n",
    "    Args:\n",
    "        shortcuts_df: DataFrame with shortcuts containing via_cell column\n",
    "        current_resolution: The H3 resolution level to use for cell containers\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added 'current_cell' column containing the parent cell\n",
    "    \"\"\"\n",
    "    \n",
    "    @F.udf(StringType())\n",
    "    def get_parent_cell(cell, target_resolution):\n",
    "        \"\"\"\n",
    "        Get the parent cell of a given cell at the target resolution.\n",
    "        Returns None if the cell is None or if the resolution is invalid.\n",
    "        \"\"\"\n",
    "        if cell is None:\n",
    "            return None\n",
    "        try:\n",
    "            cell_res = h3.get_resolution(cell)\n",
    "            # If current resolution is higher than cell resolution, return the cell itself\n",
    "            if target_resolution >= cell_res:\n",
    "                return cell\n",
    "            # Otherwise, get the parent at the target resolution\n",
    "            return h3.cell_to_parent(cell, target_resolution)\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    # Add the current_cell column based on via_cell\n",
    "    shortcuts_df = shortcuts_df.withColumn(\n",
    "        \"current_cell\",\n",
    "        get_parent_cell(F.col(\"via_cell\"), F.lit(current_resolution))\n",
    "    )\n",
    "    shortcuts_df = shortcuts_df.drop(\"lca_res\", \"via_cell\", \"via_res\")\n",
    "    # Filter out rows where current_cell is None (invalid cells)\n",
    "    # shortcuts_df = shortcuts_df.filter(F.col(\"current_cell\").isNotNull())\n",
    "    \n",
    "    return shortcuts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde5a75f",
   "metadata": {},
   "source": [
    "# 7- shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd78aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def converging_check(old_paths: DataFrame, new_paths: DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if two DataFrames of paths are identical.\n",
    "    Used to determine convergence in the shortest path algorithm.\n",
    "    \"\"\" \n",
    "    #old_paths = old_paths.reset_index(drop=True, inplace=False)\n",
    "    #new_paths = new_paths.reset_index(drop=True, inplace=False)\n",
    "    # Join on all columns to see if they match\n",
    "    \n",
    "    # Perform a left merge with indicator\n",
    "    merged_df = pd.merge(new_paths, old_paths, on=[\"incoming_edge\",\"outgoing_edge\",\"cost\"], how='left', indicator=True)\n",
    "\n",
    "    # Filter for 'left_only' rows\n",
    "    anti_left_join_df = merged_df[merged_df['_merge'] == 'left_only']\n",
    "\n",
    "    \n",
    "    #joined = new_paths.join(\n",
    "    #    old_paths,\n",
    "    #    on=[\"incoming_edge\",\"outgoing_edge\",\"cost\"],\n",
    "    #    how=\"anti_left\"\n",
    "    #)\n",
    "    return  len(anti_left_join_df)==0  #(len(joined)==0)\n",
    "    \n",
    "def compute_shortest_paths_per_partition(\n",
    "    df_shortcuts: DataFrame,\n",
    "    partition_columns: list,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes all-pairs shortest paths using vectorized pandas operations.\n",
    "    Uses Spark's applyInPandas to process each partition with optimized pandas code.\n",
    "    \n",
    "    This approach is significantly faster than pure Spark SQL for small groups\n",
    "    because it avoids shuffle operations and uses pandas' C-optimized operations.\n",
    "    \n",
    "    Args:\n",
    "        df_shortcuts: DataFrame with schema (incoming_edge, outgoing_edge, cost, via_edge)\n",
    "        partition_columns: List of column names to group by\n",
    "        max_iterations_per_group: Maximum iterations per group (default: group size)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with computed shortest paths, preserving partition columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Infer the schema for output\n",
    "    \n",
    "    output_schema = StructType([\n",
    "        StructField(\"incoming_edge\", StringType(), False),\n",
    "        StructField(\"outgoing_edge\", StringType(), False),\n",
    "        StructField(\"via_edge\", StringType(), False),\n",
    "        StructField(\"cost\", DoubleType(), False),\n",
    "    ])\n",
    "    \n",
    "    def process_partition_pandas(pdf):\n",
    "        \"\"\"\n",
    "        Process a single partition using vectorized pandas operations.\n",
    "        This function runs inside each Spark executor on the partition's data.\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        \n",
    "        if len(pdf) == 0:\n",
    "            return pd.DataFrame(columns=['incoming_edge', 'outgoing_edge','via_edge', 'cost'] )\n",
    "        \n",
    "        \n",
    "        # Start with the input edges as initial paths\n",
    "        paths = pdf[['incoming_edge', 'outgoing_edge', 'via_edge', 'cost']].copy()\n",
    "        #paths.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        \n",
    "        # Remove duplicates, keeping minimum cost\n",
    "        # No need to remove duplicates\n",
    "        paths = paths.loc[paths.groupby(['incoming_edge', 'outgoing_edge'])['cost'].idxmin()].reset_index(drop=True)\n",
    "        \n",
    "        # Set max iterations\n",
    "        #max_iters = max_iterations_per_group if max_iterations_per_group else len(pdf)\n",
    "        \n",
    "        #for iteration in range(max_iters):\n",
    "        while True:\n",
    "            # Vectorized merge: if A->B and B->C exist, create A->C\n",
    "            # This is equivalent to the self-join in Spark but uses pandas merge\n",
    "            new_paths = paths.merge(\n",
    "                paths,\n",
    "                left_on='outgoing_edge',\n",
    "                right_on='incoming_edge',\n",
    "                suffixes=('_L', '_R')\n",
    "            )\n",
    "            \n",
    "            # Calculate new costs and keep the first hop's next_node\n",
    "            new_paths = new_paths[['incoming_edge_L', 'outgoing_edge_R', 'cost_L', 'cost_R', 'outgoing_edge_L']]\n",
    "            new_paths['cost'] = new_paths['cost_L'] + new_paths['cost_R']\n",
    "            new_paths = new_paths.rename(columns={\n",
    "                'incoming_edge_L': 'incoming_edge',\n",
    "                'outgoing_edge_R': 'outgoing_edge',\n",
    "                'outgoing_edge_L': 'via_edge'\n",
    "            })[['incoming_edge', 'outgoing_edge', 'via_edge', 'cost']]\n",
    "            \n",
    "            # Filter out self-loops\n",
    "            new_paths = new_paths[new_paths['incoming_edge'] != new_paths['outgoing_edge']]\n",
    "            \n",
    "            if len(new_paths) == 0:\n",
    "                break  # No new paths to add\n",
    "            \n",
    "            # Combine existing and new paths\n",
    "            combined = pd.concat([paths, new_paths], ignore_index=True)\n",
    "            \n",
    "            # Keep only minimum cost for each (src, dst) pair - vectorized operation\n",
    "            updated_paths = combined.loc[\n",
    "                combined.groupby(['incoming_edge', 'outgoing_edge'])['cost'].idxmin()\n",
    "            ].reset_index(drop=True)\n",
    "            \n",
    "            # Check convergence: if no change in paths, we're done\n",
    "            if converging_check(paths, updated_paths):\n",
    "                break  # Converged\n",
    "            \n",
    "            paths = updated_paths\n",
    "        \n",
    "        \n",
    "        return paths\n",
    "    \n",
    "    # Apply the pandas function to each partition group\n",
    "    result = df_shortcuts.groupBy(partition_columns).applyInPandas(\n",
    "        process_partition_pandas,\n",
    "        schema=output_schema\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d79b9",
   "metadata": {},
   "source": [
    "# 8- merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "91d51537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_shortcuts_to_main_table(\n",
    "    main_df: DataFrame,\n",
    "    new_shortcuts: DataFrame,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Merges newly computed shortcuts back into the main table.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Remove old shortcuts for the processed partitions\n",
    "    2. Add new shortcuts\n",
    "    \n",
    "    Args:\n",
    "        main_df: Main shortcut table\n",
    "        new_shortcuts: Newly computed shortcuts\n",
    "        partition_columns: Columns used for partitioning\n",
    "    \n",
    "    Returns:\n",
    "        Updated DataFrame with new shortcuts\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove old shortcuts for these partitions using left_anti join\n",
    "    #\n",
    "    # insteed of using this join function, we can just use the part of main_df that filtered out\n",
    "    main_df = main_df.select(\"incoming_edge\", \"outgoing_edge\", \"cost\", \"via_edge\")\n",
    "    new_shortcuts = new_shortcuts.select(\"incoming_edge\", \"outgoing_edge\", \"cost\", \"via_edge\")\n",
    "\n",
    "    remaining_df = main_df.alias(\"main\").join(\n",
    "        new_shortcuts.alias(\"update\"),\n",
    "        on=(\n",
    "            (F.col(\"main.incoming_edge\") == F.col(\"update.incoming_edge\")) &\n",
    "            (F.col(\"main.outgoing_edge\") == F.col(\"update.outgoing_edge\"))\n",
    "        ),\n",
    "        how=\"left_anti\"\n",
    "    )    \n",
    "    \n",
    "    # Combine with new shortcuts\n",
    "    updated_df = remaining_df.unionByName(new_shortcuts)\n",
    "    \n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "514f7c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/18 13:35:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = initialize_spark()\n",
    "edges_df = read_edges(spark, file_path=\"data/burnaby_driving_simplified_edges_with_h3.csv\").cache()\n",
    "edges_cost_df = update_dummy_costs_for_edges(spark, file_path=\"data/burnaby_driving_simplified_edges_with_h3.csv\", edges_df=edges_df)\n",
    "shortcuts_df = initial_shortcuts_table(spark, file_path=\"data/burnaby_driving_edge_graph.csv\", edges_cost_df=edges_cost_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "489b05e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------------+-------+---------------+-------+\n",
      "|       incoming_edge|       outgoing_edge|            via_edge|              cost|lca_res|       via_cell|via_res|\n",
      "+--------------------+--------------------+--------------------+------------------+-------+---------------+-------+\n",
      "|(8703571310, 8703...|(8703571308, 8703...|(8703571308, 8703...|1.7882000000000002|     10|8f28de881760406|     15|\n",
      "|(7314138295, 6092...|(6092498237, 7314...|(6092498237, 7314...|             0.992|     11|8f28de8886c3b50|     15|\n",
      "|(9068804417, 9068...|(9068804425, 9463...|(9068804425, 9463...|3.4905999999999997|     11|8f28de8982e6759|     15|\n",
      "|(9289423408, 9289...|(9289423412, 9289...|(9289423412, 9289...|            2.6675|     11|8f28de1228c9309|     15|\n",
      "|(416104062, 34741...|(347415013, 25364...|(347415013, 25364...|1.9208666666666665|      9|8f28de898932612|     15|\n",
      "+--------------------+--------------------+--------------------+------------------+-------+---------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of shortcut in resolution 14 : 97963\n"
     ]
    }
   ],
   "source": [
    "for current_resolution in range(14,13,-1):\n",
    "    shortcuts_df = add_info_for_shortcuts(spark, shortcuts_df, edges_df) \n",
    "    shortcuts_df = shortcuts_df.checkpoint().cache()\n",
    "    shortcuts_df.show(5)\n",
    "    shortcuts_df_filtered =filter_shortcuts_by_resolution(shortcuts_df,current_res=current_resolution)\n",
    "    shortcuts_df_with_cell =add_parent_cell_at_resolution(shortcuts_df_filtered, current_resolution)\n",
    "    shortcuts_df_new = compute_shortest_paths_per_partition( shortcuts_df_with_cell, [\"current_cell\"])\n",
    "    #shortcuts_df = merge_shortcuts_to_main_table(shortcuts_df, shortcuts_df_new)\n",
    "    print(f\"number of shortcut in resolution {current_resolution} : {shortcuts_df_new.count()}\")\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "570f24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf91b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_sh = sh.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
